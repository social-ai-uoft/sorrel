from gem.utils import (
    updateEpsilon,
    updateMemories,
    findMoveables,
    transferWorldMemories,
    findAgents,
)

from models.memory import Memory
from models.dqn import DQN, modelDQN
from models.randomActions import modelRandomAction
from models.cnn_lstm_dqn import model_CNN_LSTM_DQN

from gemworld.gemsWolves_experimental import WolfsAndGems

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from astropy.visualization import make_lupton_rgb

import torch.nn as nn
import torch.nn.functional as F

from collections import deque

from DQN_utils import createVideo, save_models, load_models

import random
import torch

torch.manual_seed(0)
random.seed(4)

save_dir = "/Users/wil/Dropbox/Mac/Documents/gemOutput_experimental/"


def createModels():
    """
    Should make the sequence length of the LSTM part of the model and an input here
    Should also set up so that the number of hidden laters can be added to dynamically
    in this function. Below should fully set up the NN in a flexible way for the studies
    """
    models = []
    models.append(model_CNN_LSTM_DQN(5, 0.0001, 3000, 650, 425, 125, 4))  # agent model
    models.append(model_CNN_LSTM_DQN(5, 0.0001, 3000, 2570, 425, 125, 4))  # wolf model
    return models


worldSize = 15
epochs = 50000
maxTurns = 100

trainableModels = [0, 1]
sync_freq = 500
modelUpdate_freq = 25
epsilon = 0.99

turn = 1

models = createModels()
env = WolfsAndGems(worldSize, worldSize)
env.gameTest()


def runGame(
    models, env, turn, epsilon, epochs=10000, filename="filename", createVideos=False
):
    losses = 0
    gamePoints = [0, 0]
    for epoch in range(epochs):
        done, withinTurn = 0, 0

        env.reset_env(worldSize, worldSize)
        for i, j in findMoveables(env.world):
            # reset the memories for all agents
            env.world[i, j, 0].init_replay(5)

        while done == 0:
            turn = turn + 1
            withinTurn = withinTurn + 1

            if epoch % sync_freq == 0:
                for mods in trainableModels:
                    models[mods].model2.load_state_dict(
                        models[mods].model1.state_dict()
                    )

            # note, the input is not working properly to build the sequence for LSTM
            gamePoints = env.step(models, gamePoints)

            if len(trainableModels) > 0:
                # transfer the events for each agent into the appropriate model after all have moved
                env.world = updateMemories(
                    models, env.world, findMoveables(env.world), endUpdate=True
                )
                models = transferWorldMemories(
                    models, env.world, findMoveables(env.world)
                )

                if withinTurn % modelUpdate_freq == 0:
                    for mods in trainableModels:
                        loss = models[mods].training(150, 0.9)
                        losses = losses + loss.detach().numpy()

            if withinTurn > maxTurns or len(findAgents(env.world)) == 0:
                done = 1

        epsilon = updateEpsilon(epsilon, turn, epoch)
        createVideos = False
        if epoch % 100 == 0 and len(trainableModels) > 0:
            print(epoch, withinTurn, gamePoints, losses, epsilon)
            gamePoints = [0, 0]
            losses = 0
        if epoch % 10000 == 0 and createVideos == True:

            for video_num in range(5):
                vfilename = (
                    save_dir
                    + filename
                    + "_replayVid_"
                    + str(epoch)
                    + "_"
                    + str(video_num)
                    + ".gif"
                )
                createVideo(models, worldSize, 100, env, filename=vfilename)
    return models, env, turn, epsilon


models, env = runGame(
    models,
    env,
    turn,
    epsilon,
    epochs=10000,
    filename="currentWorkB",
    createVideos=False,
)

save_models(models, save_dir, "newModel10000")
