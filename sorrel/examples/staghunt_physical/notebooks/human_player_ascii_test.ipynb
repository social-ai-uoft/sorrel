{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Player Test for ASCII Map Environment\n",
    "\n",
    "This notebook demonstrates how to use the human player with the new ASCII map-based environment generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# sorrel imports\n",
    "from sorrel.examples.staghunt_physical.agents_v2 import (\n",
    "    StagHuntAgent,\n",
    "    StagHuntObservation,\n",
    ")\n",
    "from sorrel.examples.staghunt_physical.entities import Empty, entity_list\n",
    "from sorrel.examples.staghunt_physical.env import StagHuntEnv\n",
    "from sorrel.examples.staghunt_physical.world import StagHuntWorld\n",
    "from sorrel.action.action_spec import ActionSpec\n",
    "from sorrel.models.human_player import HumanPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human player test for ASCII map environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load ASCII map configuration\n",
    "    config = OmegaConf.load(\"../configs/config.yaml\")\n",
    "\n",
    "    # Create world with ASCII map generation\n",
    "    world = StagHuntWorld(config=config, default_entity=Empty())\n",
    "    experiment = StagHuntEnv(world, config)\n",
    "\n",
    "    print(f\"World dimensions: {world.height}x{world.width}\")\n",
    "    print(f\"Number of agents: {len(experiment.agents)}\")\n",
    "    print(f\"Agent spawn points: {len(world.agent_spawn_points)}\")\n",
    "\n",
    "    # Create observation spec with dynamic dimensions\n",
    "    observation_spec = StagHuntObservation(\n",
    "        entity_list=entity_list,\n",
    "        full_view=True,\n",
    "        env_dims=(world.height, world.width),  # Use actual world dimensions\n",
    "    )\n",
    "\n",
    "    # Create action spec for StagHunt environment\n",
    "    action_spec = ActionSpec(\n",
    "        [\n",
    "            \"NOOP\",\n",
    "            \"FORWARD\",\n",
    "            \"BACKWARD\",\n",
    "            \"STEP_LEFT\",\n",
    "            \"STEP_RIGHT\",\n",
    "            \"TURN_LEFT\",\n",
    "            \"TURN_RIGHT\",\n",
    "            \"ATTACK\",\n",
    "            \"PUNISH\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a custom HumanPlayer that works with StagHuntAgent\n",
    "    class StagHuntHumanPlayer(HumanPlayer):\n",
    "        def __init__(self, input_size, action_space, memory_size):\n",
    "            super().__init__(input_size, action_space, memory_size)\n",
    "\n",
    "        def take_action(self, state: np.ndarray):\n",
    "            \"\"\"Custom take_action that handles StagHuntAgent's extra features.\"\"\"\n",
    "            if self.show:\n",
    "                from IPython.display import clear_output\n",
    "\n",
    "                clear_output(wait=True)\n",
    "\n",
    "                # Simple ASCII visualization\n",
    "                print(\"Current Environment State:\")\n",
    "                print(\"=\" * 50)\n",
    "\n",
    "                # Get the world from the environment\n",
    "                if hasattr(self, \"world\") and self.world is not None:\n",
    "                    # Print basic world info\n",
    "                    print(f\"World dimensions: {self.world.height}x{self.world.width}\")\n",
    "                    print(\n",
    "                        f\"Agent location: {getattr(self, 'agent_location', 'Unknown')}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Agent orientation: {getattr(self, 'agent_orientation', 'Unknown')}\"\n",
    "                    )\n",
    "\n",
    "                    # Print inventory if available\n",
    "                    if hasattr(self, \"agent_inventory\"):\n",
    "                        print(f\"Inventory: {self.agent_inventory}\")\n",
    "                    print(\"=\" * 50)\n",
    "\n",
    "            # Get action from user with StagHunt-specific controls\n",
    "            action = None\n",
    "            num_retries = 0\n",
    "            while not isinstance(action, int):\n",
    "                action_ = input(\n",
    "                    \"Select Action (w=FORWARD, s=BACKWARD, a=TURN_LEFT, d=TURN_RIGHT, q=STEP_LEFT, e=STEP_RIGHT, r=ATTACK, p=PUNISH, 0=NOOP): \"\n",
    "                )\n",
    "                if action_ in [\"w\", \"s\", \"a\", \"d\", \"q\", \"e\", \"r\", \"p\"]:\n",
    "                    if action_ == \"w\":\n",
    "                        action = 1  # FORWARD (relative to orientation)\n",
    "                    elif action_ == \"s\":\n",
    "                        action = 2  # BACKWARD (relative to orientation)\n",
    "                    elif action_ == \"a\":\n",
    "                        action = 5  # TURN_LEFT\n",
    "                    elif action_ == \"d\":\n",
    "                        action = 6  # TURN_RIGHT\n",
    "                    elif action_ == \"q\":\n",
    "                        action = 3  # STEP_LEFT\n",
    "                    elif action_ == \"e\":\n",
    "                        action = 4  # STEP_RIGHT\n",
    "                    elif action_ == \"r\":\n",
    "                        action = 7  # ATTACK\n",
    "                    elif action_ == \"p\":\n",
    "                        action = 8  # PUNISH\n",
    "                elif action_ in [str(act) for act in self.action_list]:\n",
    "                    action = int(action_)\n",
    "                elif action_ == \"0\":\n",
    "                    action = 0  # NOOP\n",
    "                elif action_ == \"quit\":\n",
    "                    raise KeyboardInterrupt(\"Quitting...\")\n",
    "                else:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > 5:\n",
    "                        raise KeyboardInterrupt(\"Too many invalid inputs. Quitting...\")\n",
    "                    print(\"Please try again. Possible actions are below.\")\n",
    "                    print(\n",
    "                        \"Keys: w=FORWARD, s=BACKWARD, a=TURN_LEFT, d=TURN_RIGHT, q=STEP_LEFT, e=STEP_RIGHT, r=ATTACK, p=PUNISH, 0=NOOP\"\n",
    "                    )\n",
    "                    print(\"Or enter action number (0-8) or 'quit'\")\n",
    "\n",
    "            return action\n",
    "\n",
    "    # Create a custom StagHuntAgent that works with human player\n",
    "    class StagHuntHumanAgent(StagHuntAgent):\n",
    "        def get_action(self, state: np.ndarray) -> int:\n",
    "            \"\"\"Override get_action to work with human player.\"\"\"\n",
    "            # For human player, we don't need memory stacking\n",
    "            # Just pass the state directly to the model\n",
    "            action = self.model.take_action(state)\n",
    "            return action\n",
    "\n",
    "        def add_memory(\n",
    "            self, state: np.ndarray, action: int, reward: float, done: bool\n",
    "        ) -> None:\n",
    "            \"\"\"Override add_memory to handle dimension mismatch for human player.\"\"\"\n",
    "            # For human player, we don't need to store experiences in memory\n",
    "            # The human player doesn't learn from experience, so we can skip this\n",
    "            pass\n",
    "\n",
    "    # Create human player with dynamic dimensions\n",
    "    # HumanPlayer expects (height, width, channels) format\n",
    "    human_player = StagHuntHumanPlayer(\n",
    "        input_size=(world.height, world.width, 3),  # 3 layers: terrain, dynamic, beam\n",
    "        action_space=action_spec.n_actions,\n",
    "        memory_size=1,\n",
    "    )\n",
    "\n",
    "    # Create human agent\n",
    "    human_agent = StagHuntHumanAgent(\n",
    "        observation_spec=observation_spec,\n",
    "        action_spec=action_spec,\n",
    "        model=human_player,\n",
    "    )\n",
    "\n",
    "    # Replace all agents with human agent for single-player mode\n",
    "    experiment.override_agents(agents=[human_agent])\n",
    "\n",
    "    print(\"\\nStarting human player game with ASCII map environment...\")\n",
    "    print(\"Controls:\")\n",
    "    print(\"  w = FORWARD, s = BACKWARD\")\n",
    "    print(\"  a = TURN_LEFT, d = TURN_RIGHT\")\n",
    "    print(\"  q = STEP_LEFT, e = STEP_RIGHT\")\n",
    "    print(\"  r = ATTACK, p = PUNISH\")\n",
    "    print(\"  0 = NOOP, quit = Exit game\")\n",
    "    print(\"\\nPress Enter to start...\")\n",
    "    input()\n",
    "\n",
    "    experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the human player game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World dimensions: 24x25\n",
      "Number of agents: 3\n",
      "Agent spawn points: 88\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Create human player with dynamic dimensions\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Get the actual input size from the observation spec\u001b[39;00m\n\u001b[32m    123\u001b[39m full_input_dim = observation_spec.input_size[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Get the actual input size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m human_player = \u001b[43mStagHuntHumanPlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfull_input_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Create human agent\u001b[39;00m\n\u001b[32m    131\u001b[39m human_agent = StagHuntHumanAgent(\n\u001b[32m    132\u001b[39m     observation_spec=observation_spec,\n\u001b[32m    133\u001b[39m     action_spec=action_spec,\n\u001b[32m    134\u001b[39m     model=human_player,\n\u001b[32m    135\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mmain.<locals>.StagHuntHumanPlayer.__init__\u001b[39m\u001b[34m(self, input_size, action_space, memory_size)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, action_space, memory_size):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\models\\human_player.py:55\u001b[39m, in \u001b[36mHumanPlayer.__init__\u001b[39m\u001b[34m(self, input_size, action_space, memory_size, show)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Shape the input for use with the dummy memory function and the observation function.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = input_size\n\u001b[32m     52\u001b[39m _input_size = (\n\u001b[32m     53\u001b[39m     \u001b[32m1\u001b[39m,\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_size[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     56\u001b[39m     * \u001b[38;5;28mself\u001b[39m.input_size[\u001b[32m2\u001b[39m]\n\u001b[32m     57\u001b[39m     * (\u001b[38;5;28mself\u001b[39m.tile_size**\u001b[32m2\u001b[39m)\n\u001b[32m     58\u001b[39m     * \u001b[38;5;28mself\u001b[39m.num_channels,\n\u001b[32m     59\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# We will slice off the human memory zero input.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.SLICE = np.prod(_input_size)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sorrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
