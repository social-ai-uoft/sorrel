{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human Player Test for State Punishment Beta\n",
        "\n",
        "This notebook allows you to play the State Punishment Beta game as a human player. It provides:\n",
        "\n",
        "1. **Environment State Visualization**: Shows the current state of all agents in the environment\n",
        "2. **Punishment Level Display**: Shows the current punishment level and voting statistics\n",
        "3. **Agent Metrics**: Displays scalar values like rewards, social harm, individual scores, and encounters for each agent\n",
        "4. **Interactive Gameplay**: Step-by-step control with visual feedback\n",
        "\n",
        "## How to Play\n",
        "- Use WASD keys for movement (W=Up, A=Left, S=Down, D=Right)\n",
        "- Use number keys for voting actions (4=Vote Increase, 5=Vote Decrease)\n",
        "- Use 6 for No Action\n",
        "- Type 'quit' to exit\n",
        "\n",
        "## Game Mechanics\n",
        "- Collect resources (A, B, C, D, E) to gain points\n",
        "- Some resources are taboo and will cause punishment\n",
        "- Vote to increase or decrease the punishment level\n",
        "- Social harm affects all agents when taboo resources are collected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the sorrel module to path\n",
        "module_path = os.path.abspath('../../..')\n",
        "if module_path not in sys.path:\n",
        "    sys.path.insert(0, module_path)\n",
        "\n",
        "# Import sorrel components\n",
        "from sorrel.examples.state_punishment_beta.env import StatePunishmentEnv\n",
        "from sorrel.examples.state_punishment_beta.world import StatePunishmentWorld\n",
        "from sorrel.examples.state_punishment_beta.agents import StatePunishmentAgent\n",
        "from sorrel.examples.state_punishment_beta.entities import EmptyEntity\n",
        "from sorrel.models.human_player import HumanPlayer\n",
        "from sorrel.action.action_spec import ActionSpec\n",
        "from sorrel.observation.observation_spec import OneHotObservationSpec\n",
        "from sorrel.utils.visualization import plot, render_sprite\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for the game\n",
        "config = {\n",
        "    \"experiment\": {\n",
        "        \"epochs\": 1,\n",
        "        \"max_turns\": 50,\n",
        "        \"record_period\": 50,\n",
        "        \"run_name\": \"human_player_test\",\n",
        "        \"num_agents\": 3,\n",
        "        \"initial_resources\": 15,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"agent_vision_radius\": 2,\n",
        "        \"epsilon\": 0.0,  # No exploration for human player\n",
        "        \"epsilon_decay\": 0.001,\n",
        "        \"full_view\": True,\n",
        "        \"layer_size\": 128,\n",
        "        \"n_frames\": 3,\n",
        "        \"n_step\": 3,\n",
        "        \"sync_freq\": 100,\n",
        "        \"model_update_freq\": 4,\n",
        "        \"batch_size\": 64,\n",
        "        \"memory_size\": 512,\n",
        "        \"LR\": 0.00025,\n",
        "        \"TAU\": 0.001,\n",
        "        \"GAMMA\": 0.99,\n",
        "        \"n_quantiles\": 8,\n",
        "        \"device\": \"cpu\",\n",
        "    },\n",
        "    \"world\": {\n",
        "        \"height\": 10,\n",
        "        \"width\": 10,\n",
        "        \"a_value\": 3.0,\n",
        "        \"b_value\": 7.0,\n",
        "        \"c_value\": 2.0,\n",
        "        \"d_value\": -2.0,\n",
        "        \"e_value\": 1.0,\n",
        "        \"spawn_prob\": 0.05,\n",
        "        \"respawn_prob\": 0.02,\n",
        "        \"init_punishment_prob\": 0.1,\n",
        "        \"punishment_magnitude\": -10.0,\n",
        "        \"change_per_vote\": 0.2,\n",
        "        \"taboo_resources\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
        "        \"entity_spawn_probs\": {\n",
        "            \"A\": 0.2, \"B\": 0.2, \"C\": 0.2, \"D\": 0.2, \"E\": 0.2\n",
        "        }\n",
        "    },\n",
        "    \"use_composite_views\": False,\n",
        "    \"use_composite_actions\": False,\n",
        "    \"use_multi_env_composite\": False,\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom human player model for state punishment\n",
        "class StatePunishmentHumanPlayer(HumanPlayer):\n",
        "    \"\"\"Custom human player for state punishment with proper action mapping.\"\"\"\n",
        "    \n",
        "    def take_action(self, state: np.ndarray):\n",
        "        \"\"\"Override take_action to handle state punishment specific actions.\"\"\"\n",
        "        if self.show:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            # Reshape the input to return to the original image\n",
        "            state = state[:, self.SLICE:]\n",
        "            state = state.reshape(\n",
        "                (\n",
        "                    -1,\n",
        "                    self.input_size[0] * self.tile_size,\n",
        "                    self.input_size[1] * self.tile_size,\n",
        "                    self.num_channels,\n",
        "                )\n",
        "            )\n",
        "            state = np.array(state, dtype=int)\n",
        "            state_ = []\n",
        "            for i in range(state.shape[0]):\n",
        "                state_.append(state[i, :, :, :])\n",
        "            plot(state_)\n",
        "\n",
        "        action = None\n",
        "        num_retries = 0\n",
        "        while not isinstance(action, int):\n",
        "            action_ = input(\"Select Action: \")\n",
        "            \n",
        "            # Movement actions (WASD)\n",
        "            if action_ in [\"w\", \"a\", \"s\", \"d\"]:\n",
        "                if action_ == \"w\":\n",
        "                    action = 0  # Up\n",
        "                elif action_ == \"s\":\n",
        "                    action = 1  # Down\n",
        "                elif action_ == \"a\":\n",
        "                    action = 2  # Left\n",
        "                elif action_ == \"d\":\n",
        "                    action = 3  # Right\n",
        "            # Voting actions\n",
        "            elif action_ == \"4\":\n",
        "                action = 4  # Vote increase\n",
        "            elif action_ == \"5\":\n",
        "                action = 5  # Vote decrease\n",
        "            elif action_ == \"6\":\n",
        "                action = 6  # No action\n",
        "            # Direct action numbers\n",
        "            elif action_ in [str(act) for act in self.action_list]:\n",
        "                action = int(action_)\n",
        "            elif action_ == \"quit\":\n",
        "                raise KeyboardInterrupt(\"Quitting...\")\n",
        "            else:\n",
        "                num_retries += 1\n",
        "                if num_retries > 5:\n",
        "                    raise KeyboardInterrupt(\"Too many invalid inputs. Quitting...\")\n",
        "                print(\"Please try again. Possible actions:\")\n",
        "                print(\"Movement: w=Up, s=Down, a=Left, d=Right\")\n",
        "                print(\"Voting: 4=Increase punishment, 5=Decrease punishment\")\n",
        "                print(\"Other: 6=No action, quit=Exit\")\n",
        "                print(f\"Or enter action number: {list(self.action_list)}\")\n",
        "\n",
        "        return action\n",
        "\n",
        "print(\"Custom human player created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom human agent that bypasses memory stacking\n",
        "class StatePunishmentHumanAgent(StatePunishmentAgent):\n",
        "    \"\"\"Custom human agent for state punishment that bypasses memory stacking.\"\"\"\n",
        "    \n",
        "    def get_action(self, state: np.ndarray) -> int:\n",
        "        \"\"\"Override get_action to bypass memory stacking for human player.\"\"\"\n",
        "        # For human player, we don't need memory stacking\n",
        "        # Just pass the state directly to the model\n",
        "        action = self.model.take_action(state)\n",
        "        return action\n",
        "\n",
        "    def add_memory(self, state: np.ndarray, action: int, reward: float, done: bool) -> None:\n",
        "        \"\"\"Override add_memory to handle dimension mismatch for human player.\"\"\"\n",
        "        # For human player, we don't need to store experiences in memory\n",
        "        # The human player doesn't learn from experience, so we can skip this\n",
        "        pass\n",
        "\n",
        "print(\"Custom human agent created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the environment and world\n",
        "world = StatePunishmentWorld(config=config, default_entity=EmptyEntity())\n",
        "env = StatePunishmentEnv(world, config)\n",
        "\n",
        "print(f\"Environment created with {config['world']['height']}x{config['world']['width']} grid\")\n",
        "print(f\"Number of agents: {config['experiment']['num_agents']}\")\n",
        "print(f\"Taboo resources: {config['world']['taboo_resources']}\")\n",
        "print(f\"Initial punishment probability: {config['world']['init_punishment_prob']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace all agents with human players\n",
        "for i, agent in enumerate(env.agents):\n",
        "    # Create observation spec\n",
        "    entity_list = [\"EmptyEntity\", \"Wall\", \"A\", \"B\", \"C\", \"D\", \"E\", \"StatePunishmentAgent\"]\n",
        "    observation_spec = OneHotObservationSpec(\n",
        "        entity_list,\n",
        "        full_view=config[\"model\"][\"full_view\"],\n",
        "        vision_radius=config[\"model\"][\"agent_vision_radius\"],\n",
        "        env_dims=(config[\"world\"][\"height\"], config[\"world\"][\"width\"]) if config[\"model\"][\"full_view\"] else None,\n",
        "    )\n",
        "    \n",
        "    # Create action spec\n",
        "    action_names = [\"up\", \"down\", \"left\", \"right\", \"vote_increase\", \"vote_decrease\", \"noop\"]\n",
        "    action_spec = ActionSpec(action_names)\n",
        "    \n",
        "    # Create human player model\n",
        "    human_model = StatePunishmentHumanPlayer(\n",
        "        input_size=observation_spec.input_size,\n",
        "        action_space=action_spec.n_actions,\n",
        "        memory_size=1,\n",
        "        show=True\n",
        "    )\n",
        "    \n",
        "    # Create human agent\n",
        "    human_agent = StatePunishmentHumanAgent(\n",
        "        observation_spec=observation_spec,\n",
        "        action_spec=action_spec,\n",
        "        model=human_model,\n",
        "        agent_id=i,\n",
        "        use_composite_views=False,\n",
        "        use_composite_actions=False,\n",
        "        use_multi_env_composite=False,\n",
        "    )\n",
        "    \n",
        "    # Replace the original agent\n",
        "    env.agents[i] = human_agent\n",
        "\n",
        "print(f\"Replaced {len(env.agents)} agents with human players!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to display environment state and metrics\n",
        "def display_game_state(env, turn, step_info=None):\n",
        "    \"\"\"Display the current game state with all agents and metrics.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "    \n",
        "    # Create visualization of the world\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'State Punishment Beta - Turn {turn}', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Full environment view\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.set_title('Environment State (All Agents)', fontweight='bold')\n",
        "    \n",
        "    # Create a visual representation of the world\n",
        "    world_map = np.zeros((env.world.height, env.world.width, 3))\n",
        "    \n",
        "    # Color coding:\n",
        "    # Empty: White, Wall: Black, Agent: Blue, Resources: Different colors\n",
        "    for y in range(env.world.height):\n",
        "        for x in range(env.world.width):\n",
        "            entity = env.world.map[y, x, 0]\n",
        "            if hasattr(entity, 'kind'):\n",
        "                if entity.kind == 'Wall':\n",
        "                    world_map[y, x] = [0, 0, 0]  # Black\n",
        "                elif entity.kind == 'StatePunishmentAgent':\n",
        "                    world_map[y, x] = [0, 0, 1]  # Blue\n",
        "                elif entity.kind == 'A':\n",
        "                    world_map[y, x] = [1, 0, 0]  # Red\n",
        "                elif entity.kind == 'B':\n",
        "                    world_map[y, x] = [0, 1, 0]  # Green\n",
        "                elif entity.kind == 'C':\n",
        "                    world_map[y, x] = [1, 1, 0]  # Yellow\n",
        "                elif entity.kind == 'D':\n",
        "                    world_map[y, x] = [1, 0, 1]  # Magenta\n",
        "                elif entity.kind == 'E':\n",
        "                    world_map[y, x] = [0, 1, 1]  # Cyan\n",
        "                else:\n",
        "                    world_map[y, x] = [1, 1, 1]  # White (empty)\n",
        "    \n",
        "    ax1.imshow(world_map)\n",
        "    ax1.set_xlabel('X Position')\n",
        "    ax1.set_ylabel('Y Position')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add agent labels\n",
        "    for i, agent in enumerate(env.agents):\n",
        "        if hasattr(agent, 'location') and agent.location:\n",
        "            y, x = agent.location[0], agent.location[1]\n",
        "            ax1.text(x, y, f'A{i}', ha='center', va='center', \n",
        "                   color='white', fontweight='bold', fontsize=12)\n",
        "    \n",
        "    # Plot 2: Punishment level and voting info\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.set_title('Punishment System', fontweight='bold')\n",
        "    \n",
        "    # Current punishment level\n",
        "    current_prob = env.world.state_system.prob\n",
        "    ax2.bar(['Current Punishment\\\\nProbability'], [current_prob], \n",
        "           color='red' if current_prob > 0.5 else 'orange' if current_prob > 0.2 else 'green')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.set_ylabel('Probability')\n",
        "    \n",
        "    # Add text info\n",
        "    vote_stats = env.world.state_system.get_epoch_vote_stats()\n",
        "    ax2.text(0, 0.8, f'Votes Up: {vote_stats[\"vote_up\"]}', fontsize=10)\n",
        "    ax2.text(0, 0.7, f'Votes Down: {vote_stats[\"vote_down\"]}', fontsize=10)\n",
        "    ax2.text(0, 0.6, f'Total Votes: {vote_stats[\"total_votes\"]}', fontsize=10)\n",
        "    \n",
        "    # Plot 3: Agent individual scores\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.set_title('Agent Individual Scores', fontweight='bold')\n",
        "    \n",
        "    agent_scores = [agent.individual_score for agent in env.agents]\n",
        "    agent_labels = [f'Agent {i}' for i in range(len(env.agents))]\n",
        "    bars = ax3.bar(agent_labels, agent_scores, color=['blue', 'green', 'red'][:len(agent_scores)])\n",
        "    ax3.set_ylabel('Score')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars, agent_scores):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "                f'{score:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Plot 4: Agent encounters and social harm\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.set_title('Agent Encounters & Social Harm', fontweight='bold')\n",
        "    \n",
        "    # Create a table-like display\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Prepare data for display\n",
        "    table_data = []\n",
        "    table_data.append(['Agent', 'Social Harm', 'A', 'B', 'C', 'D', 'E'])\n",
        "    \n",
        "    for i, agent in enumerate(env.agents):\n",
        "        social_harm = env.world.get_social_harm(i)\n",
        "        encounters = agent.encounters\n",
        "        row = [f'Agent {i}', f'{social_harm:.1f}']\n",
        "        for resource in ['a', 'b', 'c', 'd', 'e']:\n",
        "            row.append(str(encounters.get(resource, 0)))\n",
        "        table_data.append(row)\n",
        "    \n",
        "    # Create table\n",
        "    table = ax4.table(cellText=table_data[1:], colLabels=table_data[0], \n",
        "                     cellLoc='center', loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1, 2)\n",
        "    \n",
        "    # Style the table\n",
        "    for i in range(len(table_data[0])):\n",
        "        table[(0, i)].set_facecolor('#40466e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print additional step information if provided\n",
        "    if step_info:\n",
        "        print(f\"\\\\nStep Information:\")\n",
        "        print(f\"Action taken: {step_info.get('action', 'N/A')}\")\n",
        "        print(f\"Reward received: {step_info.get('reward', 0):.2f}\")\n",
        "        print(f\"Punishment applied: {step_info.get('punishment', 0):.2f}\")\n",
        "        print(f\"Social harm received: {step_info.get('social_harm', 0):.2f}\")\n",
        "\n",
        "print(\"Display function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main game loop\n",
        "def run_human_game():\n",
        "    \"\"\"Run the human player game with step-by-step visualization.\"\"\"\n",
        "    print(\"Starting State Punishment Beta Human Player Game!\")\n",
        "    print(\"\\\\nControls:\")\n",
        "    print(\"Movement: w=Up, s=Down, a=Left, d=Right\")\n",
        "    print(\"Voting: 4=Increase punishment, 5=Decrease punishment\")\n",
        "    print(\"Other: 6=No action, quit=Exit\")\n",
        "    print(\"\\\\nPress Enter to start...\")\n",
        "    input()\n",
        "    \n",
        "    # Reset environment\n",
        "    env.reset()\n",
        "    \n",
        "    turn = 0\n",
        "    done = False\n",
        "    \n",
        "    try:\n",
        "        while not done and turn < config['experiment']['max_turns']:\n",
        "            turn += 1\n",
        "            \n",
        "            # Display current state\n",
        "            display_game_state(env, turn)\n",
        "            \n",
        "            # Let each agent (human player) take a turn\n",
        "            for agent_idx, agent in enumerate(env.agents):\n",
        "                print(f\"\\\\n--- Agent {agent_idx}'s Turn ---\")\n",
        "                \n",
        "                # Get current state for this agent\n",
        "                state = agent.pov(env.world)\n",
        "                \n",
        "                # Get action from human player\n",
        "                action = agent.get_action(state)\n",
        "                \n",
        "                # Execute action and get reward\n",
        "                reward = agent.act(env.world, action)\n",
        "                \n",
        "                # Update individual score\n",
        "                agent.individual_score += reward\n",
        "                \n",
        "                # Collect step information\n",
        "                step_info = {\n",
        "                    'action': action,\n",
        "                    'reward': reward,\n",
        "                    'punishment': 0,  # Will be calculated by the agent\n",
        "                    'social_harm': env.world.get_social_harm(agent_idx)\n",
        "                }\n",
        "                \n",
        "                # Display step information\n",
        "                print(f\"Action: {action} ({['Up', 'Down', 'Left', 'Right', 'Vote Increase', 'Vote Decrease', 'No Action'][action]})\")\n",
        "                print(f\"Reward: {reward:.2f}\")\n",
        "                print(f\"Individual Score: {agent.individual_score:.2f}\")\n",
        "                print(f\"Social Harm: {step_info['social_harm']:.2f}\")\n",
        "                \n",
        "                # Check if done\n",
        "                if env.world.is_done:\n",
        "                    done = True\n",
        "                    break\n",
        "            \n",
        "            # Spawn new resources after all agents have moved\n",
        "            env._spawn_resources()\n",
        "            \n",
        "            # Record punishment level for this turn\n",
        "            env.world.record_punishment_level()\n",
        "            \n",
        "            # Small delay for better visualization\n",
        "            time.sleep(0.5)\n",
        "        \n",
        "        # Final display\n",
        "        display_game_state(env, turn)\n",
        "        print(\"\\\\n=== GAME OVER ===\")\n",
        "        print(f\"Final scores:\")\n",
        "        for i, agent in enumerate(env.agents):\n",
        "            print(f\"Agent {i}: {agent.individual_score:.2f}\")\n",
        "        \n",
        "        # Final punishment statistics\n",
        "        vote_stats = env.world.state_system.get_epoch_vote_stats()\n",
        "        print(f\"\\\\nFinal punishment probability: {env.world.state_system.prob:.3f}\")\n",
        "        print(f\"Total votes cast: {vote_stats['total_votes']}\")\n",
        "        print(f\"Votes to increase: {vote_stats['vote_up']}\")\n",
        "        print(f\"Votes to decrease: {vote_stats['vote_down']}\")\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\\\nGame interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\\\nError during game: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"Game loop function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the game\n",
        "run_human_game()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Game Analysis\n",
        "\n",
        "After playing the game, you can analyze the results:\n",
        "\n",
        "1. **Punishment Dynamics**: How did the punishment level change throughout the game?\n",
        "2. **Agent Behavior**: Which agents collected taboo resources and how did this affect others?\n",
        "3. **Voting Patterns**: How did the voting behavior influence the punishment system?\n",
        "4. **Social Harm**: How did social harm accumulate and affect agent scores?\n",
        "\n",
        "### Key Metrics to Consider:\n",
        "- **Individual Scores**: Each agent's total reward\n",
        "- **Social Harm**: Negative impact on other agents from taboo resource collection\n",
        "- **Punishment Probability**: Current level of punishment in the system\n",
        "- **Voting Statistics**: How agents voted to change punishment levels\n",
        "- **Resource Encounters**: Which resources each agent collected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Analyze game results\n",
        "def analyze_game_results(env):\n",
        "    \"\"\"Analyze the results of the completed game.\"\"\"\n",
        "    print(\"=== GAME ANALYSIS ===\")\n",
        "    \n",
        "    # Agent performance\n",
        "    print(\"\\\\nAgent Performance:\")\n",
        "    for i, agent in enumerate(env.agents):\n",
        "        print(f\"Agent {i}:\")\n",
        "        print(f\"  Individual Score: {agent.individual_score:.2f}\")\n",
        "        print(f\"  Social Harm Received: {env.world.get_social_harm(i):.2f}\")\n",
        "        print(f\"  Resource Encounters: {dict(agent.encounters)}\")\n",
        "        print(f\"  Vote History: {agent.vote_history}\")\n",
        "    \n",
        "    # Punishment system analysis\n",
        "    print(\"\\\\nPunishment System Analysis:\")\n",
        "    print(f\"Initial Probability: {env.world.state_system.init_prob:.3f}\")\n",
        "    print(f\"Final Probability: {env.world.state_system.prob:.3f}\")\n",
        "    print(f\"Probability Change: {env.world.state_system.prob - env.world.state_system.init_prob:.3f}\")\n",
        "    \n",
        "    vote_stats = env.world.state_system.get_epoch_vote_stats()\n",
        "    print(f\"Total Votes: {vote_stats['total_votes']}\")\n",
        "    print(f\"Votes to Increase: {vote_stats['vote_up']}\")\n",
        "    print(f\"Votes to Decrease: {vote_stats['vote_down']}\")\n",
        "    \n",
        "    if vote_stats['total_votes'] > 0:\n",
        "        print(f\"Vote Ratio (Increase/Total): {vote_stats['vote_up'] / vote_stats['total_votes']:.3f}\")\n",
        "    \n",
        "    # Transgression analysis\n",
        "    print(\"\\\\nTransgression Analysis:\")\n",
        "    transgression_stats = env.world.state_system.get_transgression_stats()\n",
        "    for resource, transgressions in transgression_stats.items():\n",
        "        if 'transgressions' in resource:\n",
        "            print(f\"{resource}: {transgressions}\")\n",
        "    \n",
        "    # Resource value analysis\n",
        "    print(\"\\\\nResource Values:\")\n",
        "    print(f\"A: {config['world']['a_value']} (Taboo: {'A' in config['world']['taboo_resources']})\")\n",
        "    print(f\"B: {config['world']['b_value']} (Taboo: {'B' in config['world']['taboo_resources']})\")\n",
        "    print(f\"C: {config['world']['c_value']} (Taboo: {'C' in config['world']['taboo_resources']})\")\n",
        "    print(f\"D: {config['world']['d_value']} (Taboo: {'D' in config['world']['taboo_resources']})\")\n",
        "    print(f\"E: {config['world']['e_value']} (Taboo: {'E' in config['world']['taboo_resources']})\")\n",
        "\n",
        "# Uncomment to run analysis after the game\n",
        "# analyze_game_results(env)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
