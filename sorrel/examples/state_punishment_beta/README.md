# State Punishment Beta - Game Rules

## Overview

The State Punishment Beta is a multi-agent reinforcement learning game that explores collective punishment, voting mechanisms, and social dynamics. Agents navigate a gridworld environment, collect resources, and participate in a democratic voting system that controls punishment probabilities for taboo resource collection.

## Game Environment

### Grid World
- **Size**: 10Ã—10 grid (configurable)
- **Layers**: 1 layer
- **Boundaries**: Walls around the perimeter
- **Duration**: 100 turns per episode (configurable)
- **Agents**: 3 agents (configurable)

### Agent Characteristics
- **Unique Identity**: Each agent has a distinct visual representation
- **Vision**: 2-tile radius around agent (configurable)
- **Movement**: 4-directional (up, down, left, right)
- **Actions**: Movement + Voting + No-op

## Resources

All five resources (A, B, C, D, E) are considered **taboo** and subject to punishment:

| Resource | Value | Social Harm | Asset | Description |
|----------|-------|-------------|-------|-------------|
| **A** | +3.0 | 0.5 | ðŸŸ¢ Gem | Moderate value, low social harm |
| **B** | +7.0 | 1.0 | ðŸŸ¡ Coin | High value, moderate social harm |
| **C** | +2.0 | 0.3 | ðŸŽ Food | Low value, low social harm |
| **D** | -2.0 | 1.5 | ðŸŽ Apple | Negative value, high social harm |
| **E** | +1.0 | 0.1 | ðŸ¦´ Bone | Low value, minimal social harm |

## Actions

### Simple Mode (7 actions)
1. **Movement**: `up`, `down`, `left`, `right`
2. **Voting**: `vote_increase`, `vote_decrease`
3. **No-op**: `noop` (do nothing)

### Composite Mode (13 actions)
- **Combined Actions**: Movement + Voting in single action
  - `up_no_vote`, `down_no_vote`, `left_no_vote`, `right_no_vote`
  - `up_increase`, `down_increase`, `left_increase`, `right_increase`
  - `up_decrease`, `down_decrease`, `left_decrease`, `right_decrease`
- **No-op**: `noop`

## Voting System

### Voting Costs
- **Cost**: -0.1 reward for each vote (both increase and decrease)
- **No Cost**: Choosing not to vote (no-op) has no cost

### Voting Effects
- **Increase Vote**: +0.2 to punishment probability
- **Decrease Vote**: -0.2 to punishment probability
- **Range**: Punishment probability is clamped between 0.0 and 1.0
- **Initial**: Starts at 0.1 (10%)

### Punishment Calculation
- **Probability**: Based on current voting level (0-100%)
- **Magnitude**: -10.0 (configurable)
- **Formula**: `punishment = magnitude Ã— punishment_probability`
- **Example**: At 50% probability, punishment = -10.0 Ã— 0.5 = -5.0

## Social Harm System

### How Social Harm Works
1. **Trigger**: When any agent collects a resource with `social_harm > 0`
2. **Distribution**: Social harm is added to ALL OTHER agents (not the collector)
3. **Accumulation**: Social harm accumulates over time
4. **Application**: Each agent receives their accumulated social harm as a reward penalty at the end of their turn
5. **Reset**: Social harm is reset to 0 after being applied

### Social Harm Values
- **Resource A**: 0.5 social harm
- **Resource B**: 1.0 social harm  
- **Resource C**: 0.3 social harm
- **Resource D**: 1.5 social harm
- **Resource E**: 0.1 social harm

## Resource Spawning

### Dynamic Spawning
- **Probability**: 5% chance per turn for each empty cell
- **Method**: EmptyEntity transition system
- **Distribution**: Equal probability (20%) for each resource type
- **Initial**: 15 resources placed randomly at game start

## Reward Structure

### Individual Rewards
- **Resource Value**: Direct value from collected resource
- **Punishment**: Negative reward if punishment is applied
- **Voting Cost**: -0.1 for each vote cast
- **Social Harm**: Penalty from other agents' resource collection

### Total Reward Formula
```
Total Reward = Resource Value + Punishment + Voting Cost + Social Harm
```

## Observation System

### Agent Observations
- **Visual Field**: 2-tile radius around agent (5Ã—5 grid)
- **Entity Types**: EmptyEntity, Wall, A, B, C, D, E, Agent0, Agent1, Agent2
- **One-Hot Encoding**: Each entity type has unique representation
- **Agent Distinction**: Each agent appears differently to others

### Extra Features
- **Punishment Level**: Current punishment probability (0.0-1.0)
- **Social Harm**: Agent's current accumulated social harm
- **Random Noise**: Random value for exploration

## Game Dynamics

### Strategic Considerations
1. **Individual vs Collective**: High-value resources provide individual benefit but create social harm
2. **Voting Strategy**: Agents must balance voting costs with punishment control
3. **Social Coordination**: Agents can influence each other's behavior through voting
4. **Resource Management**: Different resources have different risk/reward profiles

### Learning Challenges
- **Multi-Agent Coordination**: Agents must learn to cooperate despite individual incentives
- **Dynamic Environment**: Punishment levels change based on collective voting
- **Social Dilemma**: Individual benefit vs. collective welfare
- **Temporal Dependencies**: Actions have delayed consequences through social harm

## Configuration Parameters

### Key Settings
- **Initial Punishment**: 0.1 (10%)
- **Punishment Magnitude**: -10.0
- **Vote Change**: 0.2 per vote
- **Voting Cost**: -0.1 per vote
- **Spawn Probability**: 0.05 per turn
- **Max Turns**: 100
- **Number of Agents**: 3

### Advanced Features
- **Composite Views**: Agents can observe from multiple perspectives
- **Composite Actions**: Movement and voting combined
- **Multi-Environment**: Agents can interact across environments

## Win Conditions

### No Explicit Win Condition
- **Objective**: Maximize cumulative reward over episode
- **Success Metrics**: 
  - High individual reward
  - Low social harm received
  - Effective punishment management
  - Cooperative behavior emergence

