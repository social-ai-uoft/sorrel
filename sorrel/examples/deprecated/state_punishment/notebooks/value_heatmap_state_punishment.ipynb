{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46267f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --------------- #\n",
    "# region: Imports #\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(\"../../..\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "# endregion       #\n",
    "# --------------- #\n",
    "\n",
    "from examples.state_punishment.utils import (\n",
    "    init_log,\n",
    "    parse_args,\n",
    "    load_config,\n",
    "    create_models,\n",
    "    create_agents,\n",
    "    create_entities,\n",
    ")\n",
    "from examples.state_punishment import agents, entities\n",
    "from examples.state_punishment.env import state_punishment\n",
    "from examples.state_punishment.utils import inspect_the_env\n",
    "from agentarium.logging_utils import GameLogger\n",
    "from agentarium.models import human_player\n",
    "from agentarium.utils import visual_field_sprite, image_from_array\n",
    "from examples.state_punishment.state_sys import state_sys, Monitor\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cfg\n",
    "config_path = \"../configs/config_fixed_rate_no_vote.yaml\"\n",
    "\n",
    "cfg = load_config(argparse.Namespace(config=config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models and envs\n",
    "\n",
    "models = create_models(cfg)\n",
    "agents = create_agents(cfg, models)\n",
    "entities = create_entities(cfg)\n",
    "envs = []\n",
    "for i in range(len(agents)):\n",
    "    envs.append(state_punishment(cfg, [agents[i]], deepcopy(entities)))\n",
    "\n",
    "\n",
    "for ixs, agent in enumerate(agents):\n",
    "    agent.model = human_player.ModelHumanPlayer(\n",
    "        action_space=8,\n",
    "        state_size=cfg.model.iqn.parameters.state_size,\n",
    "        extra_percept_size=cfg.model.iqn.parameters.extra_percept_size,\n",
    "        memory_size=1,\n",
    "        name=f\"human {ixs}\",\n",
    "    )\n",
    "    agent.model.epsilon = 0.01\n",
    "    agent.model.load(\n",
    "        f\"../models/checkpoints/fixed_punishment_rate_0.75_twoAs_extra_percept_v2_higher_harm_gem_has_value_save_model_agent{ixs}_iRainbowModel_20241127-04111732699956.pkl\"\n",
    "    )\n",
    "    # agent.model.load(\n",
    "    #      f'../models/checkpoints/fixed_punishment_rate_0.75_twoAs_extra_percept_v2_higher_harm_gem_has_value_save_model_agent{ixs}_iRainbowModel_20241127-04111732699956.pkl')\n",
    "    # agent.model.load(\n",
    "    #      '../models/checkpoints/fixed_punishment_rate_1.0_oneAs_size15_init_spawn_0.2_agent0_iRainbowModel.pkl'\n",
    "    # )\n",
    "    # agent.model.epsilon = 0.01\n",
    "\n",
    "cfg.state_sys.prob_list = {\n",
    "    \"Gem\": cfg.state_sys.prob_list.Gem,\n",
    "    \"Coin\": cfg.state_sys.prob_list.Coin,\n",
    "    \"Bone\": cfg.state_sys.prob_list.Bone,\n",
    "}\n",
    "# initialize state system\n",
    "state_entity = state_sys(\n",
    "    cfg.state_sys.init_prob,\n",
    "    cfg.state_sys.prob_list,\n",
    "    cfg.state_sys.magnitude,\n",
    "    cfg.state_sys.taboo,\n",
    "    cfg.state_sys.change_per_vote,\n",
    ")\n",
    "state_entity.prob = ...\n",
    "\n",
    "\n",
    "done = 0\n",
    "turn = 0\n",
    "losses = 0\n",
    "game_points = [0 for _ in range(len(agents))]\n",
    "# data collection\n",
    "env_templates = []\n",
    "\n",
    "# place entities\n",
    "\n",
    "# clear the world\n",
    "for env in envs:\n",
    "    env.clear_world()\n",
    "\n",
    "# place entities in the world\n",
    "envs[0].world[7, 7, 0] = [entity for entity in entities if entity.type == \"gem\"][0]\n",
    "\n",
    "# add env to templates\n",
    "env_templates.append(copy.deepcopy(envs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target locs to place the objects\n",
    "target_locs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "world_size = env_templates[0].height\n",
    "num_reps = 10\n",
    "\n",
    "v_heatmap_lst = [[] for _ in range(len(models))]\n",
    "\n",
    "# main\n",
    "for agent in agents:\n",
    "    for env in env_templates:\n",
    "\n",
    "        # find the empty locations\n",
    "        empty_locs = []\n",
    "        for i in range(env.world.shape[0]):\n",
    "            for j in range(env.world.shape[1]):\n",
    "                if str(env.world[i, j, 0]) == \"EmptyObject\":\n",
    "                    empty_locs.append((i, j, 0))\n",
    "\n",
    "        # create an empty template for the heatmap\n",
    "        v_heatmap = np.empty((world_size, world_size))\n",
    "        v_heatmap[:] = np.nan\n",
    "\n",
    "        # iteratively place the agent on every empty location in the env,\n",
    "        # then record the model's output (Q/V/...), and store it in the\n",
    "        # heatmap template\n",
    "\n",
    "        if target_locs is not None:\n",
    "            available_locs = target_locs\n",
    "        else:\n",
    "            available_locs = empty_locs\n",
    "\n",
    "        for loc in available_locs:\n",
    "            env_ = copy.deepcopy(env)\n",
    "            # place the agent\n",
    "            env_.world[loc] = agent\n",
    "            agent.location = loc\n",
    "            # generate the agent's state\n",
    "            model_input = agent.get_model_input(\n",
    "                env_, state_sys=state_entity, state_is_composite=True, envs=envs\n",
    "            )\n",
    "\n",
    "            val_estimation_trials = []\n",
    "\n",
    "            # this loop is tailored for the model used in my experiment\n",
    "            for _ in range(num_reps):\n",
    "\n",
    "                action, Qs = agent.model.take_action(model_input, eval=True)\n",
    "                max_q = float(torch.max(Qs))\n",
    "                v = float(torch.mean(Qs, dim=0))\n",
    "                val_estimation_trials.append(v)\n",
    "\n",
    "            # store the value of interest in the heatmap\n",
    "            v_m = np.mean(val_estimation_trials)\n",
    "            v_heatmap[loc[0], loc[1]] = v_m\n",
    "\n",
    "        v_heatmap_lst.append(v_heatmap)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
