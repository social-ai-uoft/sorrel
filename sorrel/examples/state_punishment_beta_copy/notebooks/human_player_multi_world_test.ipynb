{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human Player Multi-World Test\n",
        "\n",
        "This notebook allows you to play the state punishment game with multiple agents using the same multi-world visualization system.\n",
        "\n",
        "## Features:\n",
        "- Interactive human player control\n",
        "- Multi-world visualization (2√ó3 grid layout)\n",
        "- Real-time punishment level display\n",
        "- Step-by-step visualization generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "from sorrel.examples.state_punishment_beta_copy.entities import EmptyEntity\n",
        "from sorrel.examples.state_punishment_beta_copy.env import MultiAgentStatePunishmentEnv, StatePunishmentEnv\n",
        "from sorrel.examples.state_punishment_beta_copy.world import StatePunishmentWorld\n",
        "from sorrel.examples.state_punishment_beta_copy.agents import StatePunishmentAgent\n",
        "from sorrel.observation.observation_spec import OneHotObservationSpec\n",
        "from sorrel.action.action_spec import ActionSpec\n",
        "from sorrel.models.pytorch import PyTorchIQN\n",
        "from sorrel.models.human_player import HumanPlayer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multi_world_config(num_agents=3):\n",
        "    \"\"\"Create configuration for multi-world human player test using same settings as main.py.\"\"\"\n",
        "    from sorrel.examples.state_punishment_beta_copy.config import create_config\n",
        "    \n",
        "    # Use the same configuration as main.py\n",
        "    config = create_config(\n",
        "        num_agents=num_agents,\n",
        "        epochs=1,  # Just for human play\n",
        "        use_composite_views=False,\n",
        "        use_composite_actions=False,\n",
        "        use_multi_env_composite=False,\n",
        "        simple_foraging=True,  # Allow voting to change punishment level\n",
        "        use_random_policy=False,\n",
        "        fixed_punishment_level=0.2,\n",
        "        map_size=10,\n",
        "        num_resources=20,\n",
        "        learning_rate=0.00025,\n",
        "        batch_size=64,\n",
        "        memory_size=1024,\n",
        "    )\n",
        "    \n",
        "    # Override some settings for human play\n",
        "    config[\"experiment\"][\"max_turns\"] = 50  # Longer episodes for human play\n",
        "    config[\"experiment\"][\"record_period\"] = 1  # Record every turn\n",
        "    config[\"model\"][\"seed\"] = 42  # Add seed for reproducibility\n",
        "    \n",
        "    return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiWorldHumanPlayer:\n",
        "    \"\"\"Human player for multi-world state punishment game with visualization.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_agents=3):\n",
        "        self.num_agents = num_agents\n",
        "        self.config = create_multi_world_config(num_agents)\n",
        "        self.setup_environments()\n",
        "        self.current_agent = 0  # Which agent the human controls\n",
        "        self.turn_count = 0\n",
        "        self.visualizations = []  # Store visualizations for each turn\n",
        "        \n",
        "    def setup_environments(self):\n",
        "        \"\"\"Set up the multi-agent environment with human player using proper environment setup.\"\"\"\n",
        "        from sorrel.examples.state_punishment_beta_copy.environment_setup import (\n",
        "            create_shared_state_system,\n",
        "            create_shared_social_harm,\n",
        "            create_individual_environments,\n",
        "            create_multi_agent_environment\n",
        "        )\n",
        "        \n",
        "        # Create shared state system and social harm (same as main.py)\n",
        "        shared_state_system = create_shared_state_system(\n",
        "            self.config, \n",
        "            simple_foraging=False, \n",
        "            fixed_punishment_level=0.2\n",
        "        )\n",
        "        shared_social_harm = create_shared_social_harm(self.num_agents)\n",
        "        \n",
        "        # Create individual environments (one agent per world)\n",
        "        individual_envs = create_individual_environments(\n",
        "            self.config,\n",
        "            num_agents=self.num_agents,\n",
        "            simple_foraging=True,  # Allow voting to change punishment level\n",
        "            use_random_policy=False  # We'll override the first agent with human player\n",
        "        )\n",
        "        \n",
        "        # Replace the first agent with human player\n",
        "        first_env = individual_envs[0]\n",
        "        first_agent = first_env.agents[0]\n",
        "        \n",
        "        # Get the location of the original agent before replacing\n",
        "        original_location = first_agent.location\n",
        "        \n",
        "        # Create human player model\n",
        "        human_model = HumanPlayer(\n",
        "            input_size=first_agent.observation_spec.input_size,\n",
        "            action_space=first_agent.action_spec.n_actions,\n",
        "            memory_size=self.config[\"model\"][\"memory_size\"]\n",
        "        )\n",
        "        \n",
        "        # Create new human player agent with custom get_action method\n",
        "        class HumanPlayerAgent(StatePunishmentAgent):\n",
        "            \"\"\"StatePunishmentAgent with HumanPlayer model that overrides get_action.\"\"\"\n",
        "            \n",
        "            def __init__(self, *args, **kwargs):\n",
        "                super().__init__(*args, **kwargs)\n",
        "                # Override kind to match StatePunishmentAgent for observation compatibility\n",
        "                self.kind = \"StatePunishmentAgent\"\n",
        "            \n",
        "            def get_action(self, state: np.ndarray) -> int:\n",
        "                \"\"\"Override get_action to work with HumanPlayer model.\"\"\"\n",
        "                if self.use_random_policy:\n",
        "                    return np.random.randint(0, self.action_spec.n_actions)\n",
        "                \n",
        "                # For HumanPlayer, we need to format the state differently\n",
        "                # HumanPlayer expects the state to be reshaped for visualization\n",
        "                model_input = state.reshape(1, -1)\n",
        "                action = self.model.take_action(model_input)\n",
        "                return action\n",
        "            \n",
        "            def add_memory(self, state: np.ndarray, action: int, reward: float, done: bool) -> None:\n",
        "                \"\"\"Override add_memory to work with HumanPlayer model.\n",
        "                \n",
        "                HumanPlayer has a different memory structure that's incompatible\n",
        "                with the standard state format, so we skip memory addition for human players.\n",
        "                \"\"\"\n",
        "                # Skip memory addition for human players since they don't need training\n",
        "                pass\n",
        "        \n",
        "        human_agent = HumanPlayerAgent(\n",
        "            observation_spec=first_agent.observation_spec,\n",
        "            action_spec=first_agent.action_spec,\n",
        "            model=human_model,\n",
        "            agent_id=0,\n",
        "            simple_foraging=False  # Allow voting to change punishment level\n",
        "        )\n",
        "        \n",
        "        # Place the human agent at the same location as the original agent\n",
        "        first_env.world.add(original_location, human_agent)\n",
        "        \n",
        "        # Replace the agent in the environment\n",
        "        first_env.agents[0] = human_agent\n",
        "        \n",
        "        # Create multi-agent environment\n",
        "        self.multi_agent_env = create_multi_agent_environment(\n",
        "            individual_envs=individual_envs,\n",
        "            shared_state_system=shared_state_system,\n",
        "            shared_social_harm=shared_social_harm\n",
        "        )\n",
        "    \n",
        "    def generate_visualization(self):\n",
        "        \"\"\"Generate multi-world visualization for current state.\"\"\"\n",
        "        from sorrel.utils.visualization import render_sprite, image_from_array\n",
        "        from PIL import Image, ImageDraw, ImageFont\n",
        "        \n",
        "        # Render each individual world\n",
        "        world_images = []\n",
        "        for env in self.multi_agent_env.individual_envs:\n",
        "            full_sprite = render_sprite(env.world)\n",
        "            world_img = image_from_array(full_sprite)\n",
        "            world_images.append(world_img)\n",
        "        \n",
        "        # Create 2x3 grid layout\n",
        "        rows, cols = 2, 3\n",
        "        \n",
        "        # Get dimensions of individual images\n",
        "        if world_images:\n",
        "            img_width, img_height = world_images[0].size\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "        # Create combined image\n",
        "        combined_width = cols * img_width\n",
        "        combined_height = rows * img_height\n",
        "        combined_img = Image.new('RGB', (combined_width, combined_height), (255, 255, 255))\n",
        "        \n",
        "        # Place each world image in the grid\n",
        "        for i, world_img in enumerate(world_images):\n",
        "            if i >= rows * cols:\n",
        "                break\n",
        "                \n",
        "            row = i // cols\n",
        "            col = i % cols\n",
        "            \n",
        "            x = col * img_width\n",
        "            y = row * img_height\n",
        "            \n",
        "            combined_img.paste(world_img, (x, y))\n",
        "        \n",
        "        # Add labels\n",
        "        draw = ImageDraw.Draw(combined_img)\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "        \n",
        "        # Add world labels\n",
        "        for i, world_img in enumerate(world_images):\n",
        "            if i >= rows * cols:\n",
        "                break\n",
        "                \n",
        "            row = i // cols\n",
        "            col = i % cols\n",
        "            \n",
        "            x = col * img_width + 5\n",
        "            y = row * img_height + 5\n",
        "            \n",
        "            # World label with player type\n",
        "            if i == 0:\n",
        "                label = f\"World {i+1} (HUMAN)\"\n",
        "                color = (0, 0, 255)  # Blue for human\n",
        "            else:\n",
        "                label = f\"World {i+1} (AI)\"\n",
        "                color = (0, 0, 0)  # Black for AI\n",
        "            \n",
        "            draw.text((x, y), label, fill=color, font=font)\n",
        "        \n",
        "        # Add global punishment level in bottom right corner\n",
        "        punishment_level = self.multi_agent_env.shared_state_system.prob\n",
        "        punishment_text = f\"Punishment Level: {punishment_level:.3f}\"\n",
        "        text_x = combined_width - 200\n",
        "        text_y = combined_height - 30\n",
        "        draw.text((text_x, text_y), punishment_text, fill=(255, 0, 0), font=font)\n",
        "        \n",
        "        # Add turn counter\n",
        "        turn_text = f\"Turn: {self.turn_count}\"\n",
        "        draw.text((5, combined_height - 30), turn_text, fill=(0, 0, 0), font=font)\n",
        "        \n",
        "        return combined_img\n",
        "    \n",
        "    def display_current_state(self):\n",
        "        \"\"\"Display the current multi-world state.\"\"\"\n",
        "        img = self.generate_visualization()\n",
        "        if img:\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Multi-World State - Turn {self.turn_count}\")\n",
        "            plt.show()\n",
        "            \n",
        "            # Store visualization\n",
        "            self.visualizations.append(img.copy())\n",
        "    \n",
        "    def show_action_guide(self):\n",
        "        \"\"\"Display the action guide for the human player.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üéÆ HUMAN PLAYER ACTION GUIDE\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"MOVEMENT ACTIONS:\")\n",
        "        print(\"  0: Up (W key)\")\n",
        "        print(\"  1: Down (S key)\")\n",
        "        print(\"  2: Left (A key)\")\n",
        "        print(\"  3: Right (D key)\")\n",
        "        print(\"\\nVOTING ACTIONS:\")\n",
        "        print(\"  4: No operation (do nothing)\")\n",
        "        print(\"  5: Vote to INCREASE punishment level\")\n",
        "        print(\"  6: Vote to DECREASE punishment level\")\n",
        "        print(\"\\nCONTROLS:\")\n",
        "        print(\"  ‚Ä¢ Use WASD keys OR numbers 0-6\")\n",
        "        print(\"  ‚Ä¢ Type 'quit' to exit\")\n",
        "        print(\"  ‚Ä¢ Current punishment level affects all agents\")\n",
        "        print(\"=\"*60)\n",
        "    \n",
        "    def show_current_state_info(self):\n",
        "        \"\"\"Display current social harm and punishment level.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üìä CURRENT STATE INFO\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Social harm\n",
        "        print(\"üî¥ Social Harm:\")\n",
        "        for agent_id, harm in self.multi_agent_env.shared_social_harm.items():\n",
        "            print(f\"  Agent {agent_id + 1}: {harm:.3f}\")\n",
        "        \n",
        "        # Punishment level\n",
        "        print(f\"\\n‚öñÔ∏è  Punishment Level: {self.multi_agent_env.shared_state_system.prob:.3f}\")\n",
        "        \n",
        "        # Agent scores\n",
        "        print(\"\\nüí∞ Agent Scores:\")\n",
        "        for i, env in enumerate(self.multi_agent_env.individual_envs):\n",
        "            agent = env.agents[0]\n",
        "            print(f\"  Agent {i + 1}: {agent.individual_score:.3f}\")\n",
        "        \n",
        "        print(\"=\"*50)\n",
        "    \n",
        "    def play_turn(self):\n",
        "        \"\"\"Play one turn of the game.\"\"\"\n",
        "        # Display current state\n",
        "        self.display_current_state()\n",
        "        \n",
        "        print(f\"\\nTURN {self.turn_count + 1} - HUMAN PLAYER'S TURN\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Current punishment level:\", f\"{self.multi_agent_env.shared_state_system.prob:.3f}\")\n",
        "        \n",
        "        # Show action guide on first turn\n",
        "        if self.turn_count == 0:\n",
        "            self.show_action_guide()\n",
        "        \n",
        "        print(\"\\nThe HumanPlayer model will now prompt you for input...\")\n",
        "        \n",
        "        # Capture state before the turn\n",
        "        social_harm_before = self.multi_agent_env.shared_social_harm.copy()\n",
        "        punishment_before = self.multi_agent_env.shared_state_system.prob\n",
        "        \n",
        "        # Execute the turn - HumanPlayer will handle input automatically\n",
        "        self.multi_agent_env.take_turn()\n",
        "        \n",
        "        # Capture state after the turn\n",
        "        social_harm_after = self.multi_agent_env.shared_social_harm.copy()\n",
        "        punishment_after = self.multi_agent_env.shared_state_system.prob\n",
        "        agent_rewards = {}\n",
        "        \n",
        "        # Get individual agent rewards and scores\n",
        "        for i, env in enumerate(self.multi_agent_env.individual_envs):\n",
        "            agent = env.agents[0]\n",
        "            agent_rewards[f\"Agent {i+1}\"] = {\n",
        "                \"individual_score\": agent.individual_score,\n",
        "                \"last_action\": agent.last_action if hasattr(agent, 'last_action') else \"N/A\"\n",
        "            }\n",
        "        \n",
        "        # Display turn results\n",
        "        print(f\"\\nüìä TURN {self.turn_count + 1} RESULTS:\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Social harm changes\n",
        "        print(\"üî¥ Social Harm Changes:\")\n",
        "        for agent_id, harm_after in social_harm_after.items():\n",
        "            harm_before = social_harm_before.get(agent_id, 0.0)\n",
        "            harm_change = harm_after - harm_before\n",
        "            if harm_change != 0:\n",
        "                print(f\"  Agent {agent_id + 1}: {harm_before:.3f} ‚Üí {harm_after:.3f} (Œî{harm_change:+.3f})\")\n",
        "            else:\n",
        "                print(f\"  Agent {agent_id + 1}: {harm_after:.3f} (no change)\")\n",
        "        \n",
        "        # Agent rewards and actions\n",
        "        print(\"\\nüí∞ Agent Rewards & Actions:\")\n",
        "        for agent_name, info in agent_rewards.items():\n",
        "            action_names = [\"Up\", \"Down\", \"Left\", \"Right\", \"Noop\", \"Vote+\", \"Vote-\"]\n",
        "            action_name = action_names[info[\"last_action\"]] if isinstance(info[\"last_action\"], int) and 0 <= info[\"last_action\"] < 7 else \"Unknown\"\n",
        "            print(f\"  {agent_name}: Score={info['individual_score']:.3f}, Action={action_name}\")\n",
        "        \n",
        "        # Updated punishment level\n",
        "        punishment_change = punishment_after - punishment_before\n",
        "        print(f\"\\n‚öñÔ∏è  Punishment Level: {punishment_before:.3f} ‚Üí {punishment_after:.3f} (Œî{punishment_change:+.3f})\")\n",
        "        \n",
        "        self.turn_count += 1\n",
        "        \n",
        "        # Check if game is over\n",
        "        if self.turn_count >= self.config[\"experiment\"][\"max_turns\"]:\n",
        "            print(\"\\nGame Over! Maximum turns reached.\")\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def play_game(self):\n",
        "        \"\"\"Play the complete game.\"\"\"\n",
        "        print(f\"üéÆ Starting Multi-World Human Player Game with {self.num_agents} agents!\")\n",
        "        print(f\"You control World 1 (HUMAN), other worlds are controlled by AI.\")\n",
        "        print(f\"Maximum turns: {self.config['experiment']['max_turns']}\")\n",
        "        print(\"\\nüí° TIP: Type 'help' during the game to see the action guide again!\")\n",
        "        \n",
        "        while self.play_turn():\n",
        "            pass\n",
        "        \n",
        "        print(\"\\nFinal Results:\")\n",
        "        print(f\"Total turns played: {self.turn_count}\")\n",
        "        print(f\"Final punishment level: {self.multi_agent_env.shared_state_system.prob:.3f}\")\n",
        "        \n",
        "        # Display final state\n",
        "        self.display_current_state()\n",
        "        \n",
        "        return self.visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéÆ HUMAN PLAYER ACTION GUIDE\n",
            "============================================================\n",
            "MOVEMENT ACTIONS:\n",
            "  0: Up (W key)\n",
            "  1: Down (S key)\n",
            "  2: Left (A key)\n",
            "  3: Right (D key)\n",
            "\n",
            "VOTING ACTIONS:\n",
            "  4: No operation (do nothing)\n",
            "  5: Vote to INCREASE punishment level\n",
            "  6: Vote to DECREASE punishment level\n",
            "\n",
            "CONTROLS:\n",
            "  ‚Ä¢ Use WASD keys OR numbers 0-6\n",
            "  ‚Ä¢ Type 'quit' to exit\n",
            "  ‚Ä¢ Current punishment level affects all agents\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Show Action Guide\n",
        "def show_action_guide():\n",
        "    \"\"\"Display the action guide for reference.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéÆ HUMAN PLAYER ACTION GUIDE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"MOVEMENT ACTIONS:\")\n",
        "    print(\"  0: Up (W key)\")\n",
        "    print(\"  1: Down (S key)\")\n",
        "    print(\"  2: Left (A key)\")\n",
        "    print(\"  3: Right (D key)\")\n",
        "    print(\"\\nVOTING ACTIONS:\")\n",
        "    print(\"  4: No operation (do nothing)\")\n",
        "    print(\"  5: Vote to INCREASE punishment level\")\n",
        "    print(\"  6: Vote to DECREASE punishment level\")\n",
        "    print(\"\\nCONTROLS:\")\n",
        "    print(\"  ‚Ä¢ Use WASD keys OR numbers 0-6\")\n",
        "    print(\"  ‚Ä¢ Type 'quit' to exit\")\n",
        "    print(\"  ‚Ä¢ Current punishment level affects all agents\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Show Current State Info (only works after game is created)\n",
        "def show_current_state_info():\n",
        "    \"\"\"Display current social harm, punishment level, and agent scores.\"\"\"\n",
        "    if 'game' in globals():\n",
        "        game.show_current_state_info()\n",
        "    else:\n",
        "        print(\"‚ùå No game created yet. Create a game first with: game = MultiWorldHumanPlayer(num_agents=3)\")\n",
        "\n",
        "# Call these functions anytime to see the guides\n",
        "show_action_guide()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Quitting...",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create and play the game\u001b[39;00m\n\u001b[32m      2\u001b[39m game = MultiWorldHumanPlayer(num_agents=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m visualizations = \u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 312\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.play_game\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmax_turns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí° TIP: Type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhelp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m during the game to see the action guide again!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.play_turn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    253\u001b[39m punishment_before = \u001b[38;5;28mself\u001b[39m.multi_agent_env.shared_state_system.prob\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Execute the turn - HumanPlayer will handle input automatically\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmulti_agent_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Capture state after the turn\u001b[39;00m\n\u001b[32m    259\u001b[39m social_harm_after = \u001b[38;5;28mself\u001b[39m.multi_agent_env.shared_social_harm.copy()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:186\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv.take_turn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m             x.transition(env.world)\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Simplified agent transition logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_agent_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Record punishment level for all environments\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.individual_envs:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:221\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv._handle_agent_transitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    218\u001b[39m     state = agent.generate_single_view(env.world, \u001b[38;5;28mself\u001b[39m.shared_state_system, \u001b[38;5;28mself\u001b[39m.shared_social_harm)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Execute agent transition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_agent_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:250\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv._execute_agent_transition\u001b[39m\u001b[34m(self, agent, env, state)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute agent transition with the given state.\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Get action from model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m action = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Execute action with shared state system and social harm\u001b[39;00m\n\u001b[32m    253\u001b[39m reward = agent.act(env.world, action, \u001b[38;5;28mself\u001b[39m.shared_state_system, \u001b[38;5;28mself\u001b[39m.shared_social_harm)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.setup_environments.<locals>.HumanPlayerAgent.get_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# For HumanPlayer, we need to format the state differently\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# HumanPlayer expects the state to be reshaped for visualization\u001b[39;00m\n\u001b[32m     67\u001b[39m model_input = state.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\models\\human_player.py:107\u001b[39m, in \u001b[36mHumanPlayer.take_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    105\u001b[39m     action = \u001b[38;5;28mint\u001b[39m(action_)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action_ == \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mQuitting...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     num_retries += \u001b[32m1\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: Quitting..."
          ]
        }
      ],
      "source": [
        "# Create and play the game\n",
        "game = MultiWorldHumanPlayer(num_agents=2)\n",
        "visualizations = game.play_game()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save visualizations as GIF\n",
        "def save_visualizations_as_gif(visualizations, filename=\"human_player_game.gif\"):\n",
        "    \"\"\"Save all visualizations as an animated GIF.\"\"\"\n",
        "    if visualizations:\n",
        "        visualizations[0].save(\n",
        "            filename,\n",
        "            save_all=True,\n",
        "            append_images=visualizations[1:],\n",
        "            duration=1000,  # 1 second per frame\n",
        "            loop=0\n",
        "        )\n",
        "        print(f\"Game saved as {filename}\")\n",
        "    else:\n",
        "        print(\"No visualizations to save.\")\n",
        "\n",
        "# Uncomment to save the game as GIF\n",
        "# save_visualizations_as_gif(visualizations, \"human_player_multi_world_game.gif\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Quitting...",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting with 5 agents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m game_5 = MultiWorldHumanPlayer(num_agents=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m visualizations_5 = \u001b[43mgame_5\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 245\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.play_game\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou control World 1 (HUMAN), other worlds are controlled by AI.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmax_turns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 225\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.play_turn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    222\u001b[39m human_action = \u001b[38;5;28mself\u001b[39m.get_human_action()\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Execute the turn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmulti_agent_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# Override the first agent's action with human input\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;28mself\u001b[39m.multi_agent_env.individual_envs[\u001b[32m0\u001b[39m].agents[\u001b[32m0\u001b[39m].last_action = human_action\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:186\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv.take_turn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m             x.transition(env.world)\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Simplified agent transition logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_agent_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Record punishment level for all environments\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.individual_envs:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:221\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv._handle_agent_transitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    218\u001b[39m     state = agent.generate_single_view(env.world, \u001b[38;5;28mself\u001b[39m.shared_state_system, \u001b[38;5;28mself\u001b[39m.shared_social_harm)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Execute agent transition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_agent_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\examples\\state_punishment_beta_copy\\env.py:250\u001b[39m, in \u001b[36mMultiAgentStatePunishmentEnv._execute_agent_transition\u001b[39m\u001b[34m(self, agent, env, state)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute agent transition with the given state.\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Get action from model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m action = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Execute action with shared state system and social harm\u001b[39;00m\n\u001b[32m    253\u001b[39m reward = agent.act(env.world, action, \u001b[38;5;28mself\u001b[39m.shared_state_system, \u001b[38;5;28mself\u001b[39m.shared_social_harm)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mMultiWorldHumanPlayer.setup_environments.<locals>.HumanPlayerAgent.get_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# For HumanPlayer, we need to format the state differently\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# HumanPlayer expects the state to be reshaped for visualization\u001b[39;00m\n\u001b[32m     67\u001b[39m model_input = state.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\sorrel\\sorrel\\models\\human_player.py:107\u001b[39m, in \u001b[36mHumanPlayer.take_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    105\u001b[39m     action = \u001b[38;5;28mint\u001b[39m(action_)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action_ == \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mQuitting...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     num_retries += \u001b[32m1\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: Quitting..."
          ]
        }
      ],
      "source": [
        "# Test with different numbers of agents\n",
        "print(\"Testing with 5 agents...\")\n",
        "game_5 = MultiWorldHumanPlayer(num_agents=5)\n",
        "visualizations_5 = game_5.play_game()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Testing with 6 agents...\")\n",
        "game_6 = MultiWorldHumanPlayer(num_agents=6)\n",
        "visualizations_6 = game_6.play_game()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sorrel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
