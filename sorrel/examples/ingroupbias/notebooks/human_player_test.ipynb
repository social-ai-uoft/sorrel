{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Player Test for Ingroup Bias Game\n",
    "\n",
    "This notebook allows you to play the ingroup bias game manually using keyboard controls.\n",
    "You can test the game mechanics, observe agent behavior, and understand the coordination dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# sorrel imports\n",
    "from sorrel.examples.ingroupbias.agents import IngroupBiasAgent\n",
    "from sorrel.examples.ingroupbias.entities import Empty, entity_list\n",
    "from sorrel.examples.ingroupbias.env import IngroupBiasEnv\n",
    "from sorrel.examples.ingroupbias.world import IngroupBiasWorld\n",
    "from sorrel.action.action_spec import ActionSpec\n",
    "from sorrel.observation.observation_spec import OneHotObservationSpec\n",
    "from sorrel.models.human_player import HumanPlayer, HumanObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Exclude problematic modules from autoreload\n",
    "%aimport -tensorboard\n",
    "%aimport -tensorboard.compat\n",
    "%aimport -tensorboard.compat.tensorflow_stub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human player test for the ingroup bias game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    world = IngroupBiasWorld(config=config, default_entity=Empty())\n",
    "    experiment = IngroupBiasEnv(world, config)\n",
    "    agents = experiment.agents\n",
    "\n",
    "    # Use actual world dimensions instead of config dimensions\n",
    "    observation_spec = HumanObservation(\n",
    "        entity_list=entity_list,\n",
    "        full_view=True,\n",
    "        env_dims=(world.height, world.width),  # Use actual world dimensions\n",
    "    )\n",
    "    action_spec = ActionSpec(\n",
    "        [\n",
    "            \"move_up\",\n",
    "            \"move_down\",\n",
    "            \"move_left\",\n",
    "            \"move_right\",\n",
    "            \"turn_left\",\n",
    "            \"turn_right\",\n",
    "            \"strafe_left\",\n",
    "            \"strafe_right\",\n",
    "            \"interact\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a custom HumanPlayer that works with IngroupBiasAgent\n",
    "    class IngroupBiasHumanPlayer(HumanPlayer):\n",
    "        def __init__(self, input_size, action_space, memory_size):\n",
    "            super().__init__(input_size, action_space, memory_size)\n",
    "            # Calculate the expected visual size (without extra features)\n",
    "            self.visual_size = (\n",
    "                input_size[0]\n",
    "                * input_size[1]\n",
    "                * input_size[2]\n",
    "                * (self.tile_size**2)\n",
    "                * self.num_channels\n",
    "            )\n",
    "            # The IngroupBiasAgent will provide visual_size + 4 extra features (inventory + ready)\n",
    "            self.total_input_size = self.visual_size + 4\n",
    "\n",
    "        def take_action(self, state: np.ndarray):\n",
    "            \"\"\"Custom take_action that handles IngroupBiasAgent's extra features.\"\"\"\n",
    "            if self.show:\n",
    "                from IPython.display import clear_output\n",
    "\n",
    "                clear_output(wait=True)\n",
    "\n",
    "                # Render the world properly with all layers\n",
    "                from sorrel.utils.visualization import render_sprite, image_from_array\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Render the world properly with all layers\n",
    "                layers = render_sprite(self.world, tile_size=[32, 32])\n",
    "\n",
    "                # Composite the layers properly\n",
    "                composited = image_from_array(layers)\n",
    "                composited_array = np.array(composited)\n",
    "                print(f\"World visualization shape: {composited_array.shape}\")\n",
    "\n",
    "                # Display the composited result\n",
    "                plt.figure(figsize=(12, 12))\n",
    "                plt.imshow(composited_array)\n",
    "                plt.title(\"Ingroup Bias Environment - Human Player Test\")\n",
    "                plt.xlabel(\"X coordinate\")\n",
    "                plt.ylabel(\"Y coordinate\")\n",
    "\n",
    "                # Add legend\n",
    "                legend_elements = [\n",
    "                    plt.Rectangle((0, 0), 1, 1, facecolor=\"gray\", label=\"Wall\"),\n",
    "                    plt.Rectangle((0, 0), 1, 1, facecolor=\"brown\", label=\"Sand\"),\n",
    "                    plt.Rectangle((0, 0), 1, 1, facecolor=\"red\", label=\"Red Resource\"),\n",
    "                    plt.Rectangle(\n",
    "                        (0, 0), 1, 1, facecolor=\"green\", label=\"Green Resource\"\n",
    "                    ),\n",
    "                    plt.Rectangle(\n",
    "                        (0, 0), 1, 1, facecolor=\"blue\", label=\"Blue Resource\"\n",
    "                    ),\n",
    "                    plt.Rectangle((0, 0), 1, 1, facecolor=\"yellow\", label=\"Agent\"),\n",
    "                    plt.Rectangle((0, 0), 1, 1, facecolor=\"white\", label=\"Empty\"),\n",
    "                ]\n",
    "                plt.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            # Get action from user\n",
    "            action = None\n",
    "            num_retries = 0\n",
    "            while not isinstance(action, int):\n",
    "                action_ = input(\n",
    "                    \"Select Action (w=up, s=down, a=left, d=right, q=turn_left, e=turn_right, z=strafe_left, c=strafe_right, r=interact, 0=noop): \"\n",
    "                )\n",
    "                if action_ in [\"w\", \"s\", \"a\", \"d\", \"q\", \"e\", \"z\", \"c\", \"r\"]:\n",
    "                    if action_ == \"w\":\n",
    "                        action = 0  # move_up\n",
    "                    elif action_ == \"s\":\n",
    "                        action = 1  # move_down\n",
    "                    elif action_ == \"a\":\n",
    "                        action = 2  # move_left\n",
    "                    elif action_ == \"d\":\n",
    "                        action = 3  # move_right\n",
    "                    elif action_ == \"q\":\n",
    "                        action = 4  # turn_left\n",
    "                    elif action_ == \"e\":\n",
    "                        action = 5  # turn_right\n",
    "                    elif action_ == \"z\":\n",
    "                        action = 6  # strafe_left\n",
    "                    elif action_ == \"c\":\n",
    "                        action = 7  # strafe_right\n",
    "                    elif action_ == \"r\":\n",
    "                        action = 8  # interact\n",
    "                elif action_ in [str(act) for act in self.action_list]:\n",
    "                    action = int(action_)\n",
    "                elif action_ == \"0\":\n",
    "                    action = 0  # noop (same as move_up for simplicity)\n",
    "                elif action_ == \"quit\":\n",
    "                    raise KeyboardInterrupt(\"Quitting...\")\n",
    "                else:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > 5:\n",
    "                        raise KeyboardInterrupt(\"Too many invalid inputs. Quitting...\")\n",
    "                    print(\"Please try again. Possible actions are below.\")\n",
    "                    print(\n",
    "                        \"Keys: w=up, s=down, a=left, d=right, q=turn_left, e=turn_right, z=strafe_left, c=strafe_right, r=interact, 0=noop\"\n",
    "                    )\n",
    "                    print(\"Or enter action number (0-8) or 'quit'\")\n",
    "\n",
    "            return action\n",
    "\n",
    "    # Create a custom IngroupBiasAgent that bypasses the memory stacking issue\n",
    "    class IngroupBiasHumanAgent(IngroupBiasAgent):\n",
    "        def get_action(self, state: np.ndarray) -> int:\n",
    "            \"\"\"Override get_action to bypass memory stacking for human player.\"\"\"\n",
    "            # For human player, we don't need memory stacking\n",
    "            # Just pass the state directly to the model\n",
    "            action = self.model.take_action(state)\n",
    "            return action\n",
    "\n",
    "        def add_memory(\n",
    "            self, state: np.ndarray, action: int, reward: float, done: bool\n",
    "        ) -> None:\n",
    "            \"\"\"Override add_memory to handle dimension mismatch for human player.\"\"\"\n",
    "            # For human player, we don't need to store experiences in memory\n",
    "            # The human player doesn't learn from experience, so we can skip this\n",
    "            pass\n",
    "\n",
    "        def can_act(self) -> bool:\n",
    "            \"\"\"Override can_act to allow human player to act even when frozen.\"\"\"\n",
    "            # Human players can act when frozen (for testing), but not when removed\n",
    "            return not self.is_removed\n",
    "\n",
    "    # Create human player\n",
    "    human_player = IngroupBiasHumanPlayer(\n",
    "        input_size=(\n",
    "            world.height,\n",
    "            world.width,\n",
    "            3,\n",
    "        ),  # 3 layers: terrain, dynamic, beam\n",
    "        action_space=action_spec.n_actions,\n",
    "        memory_size=1,\n",
    "    )\n",
    "\n",
    "    # Store world reference for visualization\n",
    "    human_player.world = world\n",
    "\n",
    "    # Create human agent\n",
    "    human_agent = IngroupBiasHumanAgent(\n",
    "        observation_spec=observation_spec,\n",
    "        action_spec=action_spec,\n",
    "        model=human_player,\n",
    "    )\n",
    "\n",
    "    # Replace one of the agents with human player\n",
    "    agents[0] = human_agent\n",
    "    experiment.override_agents(agents)\n",
    "\n",
    "    print(\"Starting Ingroup Bias Human Player Test\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Controls:\")\n",
    "    print(\"  w = move up\")\n",
    "    print(\"  s = move down\")\n",
    "    print(\"  a = move left\")\n",
    "    print(\"  d = move right\")\n",
    "    print(\"  q = turn left\")\n",
    "    print(\"  e = turn right\")\n",
    "    print(\"  z = strafe left\")\n",
    "    print(\"  c = strafe right\")\n",
    "    print(\"  r = interact\")\n",
    "    print(\"  0 = noop\")\n",
    "    print(\"  quit = exit\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nGame Rules:\")\n",
    "    print(\"- Collect resources of your color group\")\n",
    "    print(\"- Interact with other agents to coordinate\")\n",
    "    print(\"- Avoid collecting resources of other groups\")\n",
    "    print(\"- You are the YELLOW agent in the visualization\")\n",
    "    print(\"\\nStarting game...\")\n",
    "\n",
    "    experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn taking loop\n",
    "Choose an action from [0, 1, 2, 3, 4, 5, 6, 7, 8] to act on the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration file paths\n",
    "config_path = \"../configs/config_ascii_map.yaml\"  # ASCII map version\n",
    "# config_path = \"../configs/config.yaml\"  # Random generation version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Information\n",
    "\n",
    "**Objective:** Collect resources that match your group color while coordinating with other agents.\n",
    "\n",
    "**Mechanics:**\n",
    "- **Movement:** Use WASD keys to move around the environment\n",
    "- **Rotation:** Use Q/E keys to turn left/right\n",
    "- **Strafing:** Use Z/C keys to strafe left/right\n",
    "- **Interaction:** Use R key to interact with other agents\n",
    "- **No Action:** Use 0 key for no operation\n",
    "\n",
    "**Group Dynamics:**\n",
    "- Agents are assigned to color groups (red, green, blue)\n",
    "- Collecting resources of your group color gives positive reward\n",
    "- Collecting resources of other groups may give negative reward\n",
    "- Coordination with same-group agents is beneficial\n",
    "\n",
    "**Visual Legend:**\n",
    "- **Gray:** Walls (impassable)\n",
    "- **Brown:** Sand (traversable terrain)\n",
    "- **Red:** Red resources\n",
    "- **Green:** Green resources  \n",
    "- **Blue:** Blue resources\n",
    "- **Yellow:** Your agent\n",
    "- **White:** Empty spaces\n",
    "\n",
    "**Tips:**\n",
    "- Try to coordinate with other agents of your group\n",
    "- Avoid collecting resources of other groups\n",
    "- Use interaction to communicate with other agents\n",
    "- Observe how the AI agents behave and learn from their strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
