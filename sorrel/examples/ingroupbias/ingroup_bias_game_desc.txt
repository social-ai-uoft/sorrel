Paradigm Description from Köster et al. (2025)
The coordination game implemented here reproduces the foraging task described by Köster et al. (2025) for studying the emergence of group bias in reinforcement‑learning agents. The original paper describes a partially observable Markov game set in a two–dimensional grid world. Each episode begins with a population of eight agents spawned at random locations. Resources (red, green and blue) appear stochastically throughout the world; agents collect a resource by walking over it. Collecting a resource increments the corresponding entry in the agent’s inventory and gives the agent a “ready to interact” headband that signals it can now engage in social interactions.
Agents navigate the world using a discrete action space that includes movement in four directions, turning and strafing (sideways movement) as well as the ability to fire an “interaction beam”. The beam travels a limited distance (it cannot go through walls) and can only be fired by agents that are ready to interact. When the beam hits another ready agent, both agents receive a reward equal to the dot product of their three‑element inventories, encouraging them to collect similar resources before interacting. After a successful interaction, both agents freeze for 16 time‑steps and then disappear for 50 time‑steps before respawning at a starting location. Agents receive no reward for collecting resources themselves; all reward comes from interactions.
The world is filled with walls that form a boundary and may include internal obstacles. Agents have partial observability: they perceive only a limited window (e.g., 11×11 cells) centred on their current location. Training in the original study uses two separate servers: an in‑group server where all agents share the same colour and a mixed server where agents of different colours are present. The paper introduces a dual‑choice evaluation, where a test agent is placed in a small arena with two immobile, ready agents of different colours; the test agent chooses whom to interact with and thereby reveals any group bias. Bias is measured as the proportion of times agents interact with their own colour when no instrumental reason exists to prefer one over the other.
Our implementation captures the core mechanics of this paradigm: stochastic resource spawning and collection, readiness signalling via inventories, directed interaction beams with limited range and wall obstructions, reward proportional to inventory similarity, freezing and respawning after interaction, partial observability via an optional observation function, and a dual‑choice probe environment for measuring group bias.
________________________________________
