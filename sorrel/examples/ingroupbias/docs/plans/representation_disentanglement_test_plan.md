# Representation Disentanglement Test Plan

## **Core Concept: Measuring Learned Structure in Agent Representations**

The representation disentanglement test measures how well agents have learned to internally represent the latent factors of their environment through their task experience. We test whether the agent's hidden layer activations contain linearly separable representations of the original latent factors.

## **1. Test Objectives**

The primary objective is to determine if agents have learned to disentangle the latent factors (color, size, shape, pattern) in their internal representations, despite receiving only entangled input representations. We want to measure the progression of this learning over training epochs and understand how coordination demands influence the emergence of structured representations.

## **2. Test Methodology**

During tests, agents will be placed in a predetermined viewpoint, where they will observe a certain agent OR resource entity in front of them. We will extract the representations from the agent's model and then test representational disentanglement.

### **Activation Extraction**
We extract hidden layer activations from specific layers of the agent's neural network during forward passes. The test focuses on the head1 and ff_1 layers of the IQN model, which are key intermediate representations. We use PyTorch hooks to capture these activations without modifying the model's forward pass.

### **Stimulus** (? find the original paper about details of how to smaple the test and training set)
We will use stimulus generated by the icon generation system, placed them in front of the focal agent. The set of required stimuli will be initially generated when the main experiment is started. We will generate a set of stimuli used in the training and another set used in this testing stage. Specifically, we will randomly sample X points from the distribution of each latent factors, and then randomly group them, which results in X set of latent profiles. We will do the same procedure for both resources and agents. We will then divide it into test and training set, with a ratio of 1:1. The testing set will be used here. Every trial, we will pick one tuple of latent factors, and use the encoder (from the icon generation system) to generate the icon, and collect agents' representations of the icon. 

### **Data Collection Process**
For each agent, we collect hidden activations by presenting them with different entity configurations. We record the activations from the target layers along with the ground truth latent factor values. This creates a dataset of hidden representations paired with their corresponding latent factor values. The result dataframe should include columns as follows: id of the tested agent, trial number, ground truth latent factor values (each latent factor has one column), test result.

## **3. Linear Separability Testing** (? )

### **Logistic Regression Analysis**
We train logistic regression classifiers to predict each latent factor from the hidden activations. For each factor (color, size, shape, pattern), we train a separate classifier using the collected activations as input and the factor values as targets. We use cross-validation to ensure robust evaluation.

### **Accuracy Measurement**
We measure classification accuracy for each latent factor. High accuracy indicates that the factor is well-represented and linearly separable in the hidden activations. Low accuracy suggests the factor remains entangled or poorly represented. We also measure overall accuracy across all factors.

### **Entanglement Scoring**
We calculate an entanglement score as one minus the mean accuracy across all non-type factors. A high entanglement score indicates that the factors are poorly separable and remain entangled. A low entanglement score suggests successful disentanglement learning.

## **4. Test Protocol**

### **Epoch-Based Testing**
The test runs at regular intervals during training, typically every 2000 epochs. This allows us to track the progression of disentanglement learning over time and understand how it relates to task performance and coordination demands.


### **Layer-Specific Analysis**
We test multiple layers of the neural network to understand where disentanglement occurs. Different layers may show different levels of factor separation, revealing the hierarchical organization of learned representations.

## **5. Data Collection and Storage**

### **CSV Output Format**
Test results are saved as CSV files with columns for experiment name, test index, epoch, agent ID, layer name, factor name, accuracy, and additional metadata. Each test run generates a separate file to enable detailed analysis and comparison.

### **Structured Data Organization**
Results are organized by experiment name and test index, making it easy to track results across multiple test runs and compare different experimental conditions. The file naming convention includes experiment name, agent ID, epoch, and test type.

### **Metadata Recording**
We record comprehensive metadata including model architecture details, training parameters, test configuration, and environmental conditions. This ensures reproducibility and enables detailed analysis of factors influencing disentanglement.

## **6. Validation and Quality Control**

### **Cross-Validation**
We use k-fold cross-validation to ensure robust accuracy measurements. This prevents overfitting to specific test samples and provides confidence intervals for our measurements.

### **Statistical Significance**
We perform statistical tests to determine if observed accuracy differences are significant. This helps distinguish between meaningful learning and random variation in the disentanglement measurements.

### **Baseline Comparisons**
We compare results against baseline measurements from untrained models and random representations. This provides context for interpreting the disentanglement scores and understanding the magnitude of learning effects.

## **7. Integration with Training Loop**

### **Automatic Testing**
The test is automatically triggered at specified epoch intervals during training. This ensures consistent testing without manual intervention and allows for long-running experiments with regular monitoring.

### **Non-Disruptive Design**
The test is designed to be non-disruptive to the training process. It uses frozen model states and doesn't interfere with the agent's learning or decision-making during episodes.

### **Efficient Execution**
The test is optimized for efficiency to minimize computational overhead. It uses batch processing where possible and caches intermediate results to reduce redundant computations.



