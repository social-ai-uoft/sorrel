{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model + RPG testing pipeline\n",
    "\n",
    "This document lays out the procedure for:\n",
    "- Training the forward model to learn the RPG environment and saving it to disk\n",
    "- Using the trained forward model to generate expert memories and saving them\n",
    "- Loading the expert memories into the transformer model\n",
    "- Training the transformer model\n",
    "\n",
    "## Table of Contents\n",
    "1. [Train the forward model](#Train-the-forward-model)\n",
    "2. [Locate the saved model](#Locate-the-saved-model)\n",
    "3. [Generate memories and save to disk](#Generate-memories-and-save-to-disk)\n",
    "4. [Evaluate the transformer model](#Evaluate-the-transformer-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# region: Imports #\n",
    "# --------------- #\n",
    "\n",
    "# Import base packages\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sorrel.examples.treasurehunt.entities import EmptyEntity\n",
    "from sorrel.examples.treasurehunt.env import TreasurehuntEnv\n",
    "from sorrel.examples.treasurehunt.world import TreasurehuntWorld\n",
    "from sorrel.utils.logging import TensorboardLogger\n",
    "\n",
    "# --------------- #\n",
    "# endregion       #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the forward model\n",
    "\n",
    "To give the transformer model something to learn, we need to first train a model to solve the RPG task.\n",
    "\n",
    "To edit details of the model training regime, use the configuration file (by default, stored in `../configs/config.yaml` relative to this Python notebook) to change them. Some details need to be changed together for the model to successfully run.\n",
    "\n",
    "In addition to providing a simple console log summary of each epoch, more detailed data is stored in TensorBoard if `log` in the configuration file is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "Model saved to: ../data/checkpoints/treasurehunt_model_2025-12-22_16-55-42.pkl\n"
     ]
    }
   ],
   "source": [
    "STATIC_RUNTIME = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Load configuration\n",
    "config = {\n",
    "    \"experiment\": {\n",
    "        \"epochs\": 1000,\n",
    "        \"max_turns\": 100,\n",
    "        \"record_period\": 50,\n",
    "        \"log_dir\": Path(\"../data/logs/forward_model\") / STATIC_RUNTIME,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"agent_vision_radius\": 4,\n",
    "        \"epsilon_decay\": 0.0005,\n",
    "    },\n",
    "    \"world\": {\n",
    "        \"height\": 20,\n",
    "        \"width\": 20,\n",
    "        \"gem_value\": 10,\n",
    "        \"food_value\": 5,\n",
    "        \"bone_value\": -10,\n",
    "        \"spawn_prob\": 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "# construct the world\n",
    "world = TreasurehuntWorld(config=config, default_entity=EmptyEntity())\n",
    "\n",
    "# construct the environment\n",
    "env = TreasurehuntEnv(world, config)\n",
    "\n",
    "# model save path\n",
    "model_path = Path(\"../data/checkpoints\") / f\"treasurehunt_model_{STATIC_RUNTIME}.pkl\"\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# run the experiment with default parameters\n",
    "env.run_experiment(\n",
    "    output_dir=Path(\"../data\"),\n",
    "    logger=TensorboardLogger.from_config(config),\n",
    ")\n",
    "\n",
    "# save model\n",
    "print(\"\\nSaving model...\")\n",
    "for agent in env.agents:\n",
    "    agent.model.save(file_path=str(model_path))\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate memories and save to disk\n",
    "\n",
    "Generating memories of game trajectories, replays of games, and model scores on games is achieved using the `eval_model()` function. This function plays a specified number of games (by default, 1) and returns a dictionary of output variables. The memories file is somewhat large (~ 1.5 GB for 1024 games), so keep that in mind.\n",
    "\n",
    "Flags that can be used:\n",
    "- `'memories'` returns a stored memory buffer of size (n_games x max_turns)\n",
    "- `'frames'` returns a list of size (n_games x max_turns) with images of each turn.\n",
    "- `'scores` returns a record of the model's reward on each turn.\n",
    "- `'jupyter-mode'` should be added when you are using `'frames'` from a Python notebook rather than from the command line.\n",
    "\n",
    "**NOTE**: As generating the frames takes a while, it's generally faster to generate animated replays and memories separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ../data/checkpoints/treasurehunt_model_2025-12-22_16-55-42.pkl\n",
      "\n",
      "Memories saved!\n",
      "Agent 0: ../data/memories/agent0.npz\n",
      "Agent 1: ../data/memories/agent1.npz\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(f\"../data/checkpoints/treasurehunt_model_{STATIC_RUNTIME}.pkl\")\n",
    "print(f\"Using model: {model_path}\")\n",
    "\n",
    "# Reconstructing the same enviornment\n",
    "world = TreasurehuntWorld(config=config, default_entity=EmptyEntity())\n",
    "env = TreasurehuntEnv(world, config)\n",
    "\n",
    "# Loading saved weights\n",
    "for i, agent in enumerate(env.agents):\n",
    "    agent.model.load(file_path=str(model_path))  # type: ignore\n",
    "\n",
    "# Generating memories\n",
    "output_dir = Path(\"../data\")\n",
    "num_games = 128\n",
    "\n",
    "env.generate_memories(num_games=num_games, animate=False, output_dir=output_dir)\n",
    "\n",
    "\n",
    "print(f\"\\nMemories saved!\")\n",
    "print(f\"Agent 0: {output_dir / 'memories/agent0.npz'}\")\n",
    "print(f\"Agent 1: {output_dir / 'memories/agent1.npz'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sorrel.models.pytorch.transformer import ViTOneHot\n",
    "from sorrel.buffers import TransformerBuffer\n",
    "from sorrel.utils.logging import TensorboardLogger\n",
    "\n",
    "TRAINING_EPOCHS = 1000\n",
    "STATIC_RUNTIME = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "transformer_buffer = TransformerBuffer.load(Path(\"../data\") / \"memories/agent1.npz\")\n",
    "model = ViTOneHot(\n",
    "    state_size=(6, 9, 9),\n",
    "    action_space=4,\n",
    "    layer_size=192,\n",
    "    patch_size=3,\n",
    "    num_frames=5,\n",
    "    num_heads=3,\n",
    "    batch_size=64,\n",
    "    num_layers=2,\n",
    "    memory=transformer_buffer,\n",
    "    LR=0.001,\n",
    "    device=\"cpu\",\n",
    "    seed=random.randint(1, 1000),\n",
    ")\n",
    "logger = TensorboardLogger(\n",
    "    TRAINING_EPOCHS,\n",
    "    Path(\"../data/logs/inverse_model\") / STATIC_RUNTIME,\n",
    "    \"state_loss\",\n",
    "    \"action_loss\",\n",
    "    \"state_targets\",\n",
    "    \"state_preds\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the transformer model\n",
    "\n",
    "The transformer model uses a separate configuration file (by default, `../configs/transformer.yaml`). This file is shorter and handles only the details of the transformer model itself, since all of the details of the environment were already established when training the forward model. Make sure that the input parameters are compatible with those used by the forward model.\n",
    "\n",
    "**NOTE**: Be aware that at this stage, some specific configurations are not compatible with the transformer model. For example, the transformer model requires the state space to be evenly divisible by a patch size. Since the agent vision results in an odd-numbered state H x W, the state size in the forward model must have a H x W of e.g., 9 x 9, 15 x 15, 21 x 21 in order to have a patch size of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    state_predictions, state_targets = model.plot_trajectory()\n",
    "\n",
    "    logger.record_turn(\n",
    "        epoch=epoch,\n",
    "        loss=action_loss + state_loss,\n",
    "        reward=0.0,\n",
    "        action_loss=action_loss,\n",
    "        state_loss=state_loss,\n",
    "    )\n",
    "    logger.writer.add_images(\n",
    "        \"state_targets\", state_targets[:, 1:4], epoch, dataformats=\"NCHW\"\n",
    "    )\n",
    "    logger.writer.add_images(\n",
    "        \"state_preds\", state_predictions[:, 1:4], epoch, dataformats=\"NCHW\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple test to validate weight transfer hypothesis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON SELF\n",
      "\n",
      "Evaluating on Agent 1\n",
      "Total Loss:  0.6601 ± 0.0522\n",
      "State Loss:  0.5749\n",
      "Action Loss: 0.0852\n",
      "\n",
      "CROSS-AGENT TRANSFER\n",
      "\n",
      "Evaluating on Agent 2\n",
      "Total Loss:  0.7055 ± 0.0702\n",
      "State Loss:  0.5722\n",
      "Action Loss: 0.1334\n",
      "RESULTS\n",
      "\n",
      "Transfer Gaps:\n",
      "Total Loss:    +6.9%\n",
      "State Loss:    -0.5%\n",
      "Action Loss:  +56.5%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from sorrel.buffers import TransformerBuffer\n",
    "\n",
    "# Since we already loded agent1.npz buffer as transformer_buffer\n",
    "# Load agent2 buffer with agent0.npz\n",
    "buffer_agent_2 = TransformerBuffer.load(Path(\"../data/memories/agent0.npz\"))\n",
    "\n",
    "# Evaluate on Agent 1 (Self)\n",
    "\n",
    "print(\"EVALUATING ON SELF\")\n",
    "\n",
    "model.memory = transformer_buffer\n",
    "\n",
    "model.eval()\n",
    "eval_losses_agent_1 = []\n",
    "eval_state_losses_1 = []\n",
    "eval_action_losses_1 = []\n",
    "\n",
    "print(\"\\nEvaluating on Agent 1\")\n",
    "\n",
    "for _ in range(500):\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    eval_losses_agent_1.append(state_loss + action_loss)\n",
    "    eval_state_losses_1.append(state_loss)\n",
    "    eval_action_losses_1.append(action_loss)\n",
    "\n",
    "avg_loss_agent_1 = np.mean(eval_losses_agent_1)\n",
    "std_loss_agent_1 = np.std(eval_losses_agent_1)\n",
    "avg_state_loss_1 = np.mean(eval_state_losses_1)\n",
    "avg_action_loss_1 = np.mean(eval_action_losses_1)\n",
    "\n",
    "print(f\"Total Loss:  {avg_loss_agent_1:.4f} ± {std_loss_agent_1:.4f}\")\n",
    "print(f\"State Loss:  {avg_state_loss_1:.4f}\")\n",
    "print(f\"Action Loss: {avg_action_loss_1:.4f}\")\n",
    "\n",
    "# Cross-Agent Transfer (Agent 2)\n",
    "\n",
    "print(\"\\nCROSS-AGENT TRANSFER\")\n",
    "\n",
    "# Switch to Agent 2's memory\n",
    "model.memory = buffer_agent_2\n",
    "\n",
    "eval_losses_agent_2 = []\n",
    "eval_state_losses_2 = []\n",
    "eval_action_losses_2 = []\n",
    "\n",
    "print(\"\\nEvaluating on Agent 2\")\n",
    "for _ in range(500):\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    eval_losses_agent_2.append(state_loss + action_loss)\n",
    "    eval_state_losses_2.append(state_loss)\n",
    "    eval_action_losses_2.append(action_loss)\n",
    "\n",
    "avg_loss_agent_2 = np.mean(eval_losses_agent_2)\n",
    "std_loss_agent_2 = np.std(eval_losses_agent_2)\n",
    "avg_state_loss_2 = np.mean(eval_state_losses_2)\n",
    "avg_action_loss_2 = np.mean(eval_action_losses_2)\n",
    "\n",
    "print(f\"Total Loss:  {avg_loss_agent_2:.4f} ± {std_loss_agent_2:.4f}\")\n",
    "print(f\"State Loss:  {avg_state_loss_2:.4f}\")\n",
    "print(f\"Action Loss: {avg_action_loss_2:.4f}\")\n",
    "\n",
    "# RESULTS\n",
    "\n",
    "print(\"RESULTS\")\n",
    "\n",
    "# Transfer metrics\n",
    "total_transfer_gap = (avg_loss_agent_2 - avg_loss_agent_1) / avg_loss_agent_1 * 100\n",
    "state_transfer_gap = (avg_state_loss_2 - avg_state_loss_1) / avg_state_loss_1 * 100\n",
    "action_transfer_gap = (avg_action_loss_2 - avg_action_loss_1) / avg_action_loss_1 * 100\n",
    "\n",
    "print(f\"\\nTransfer Gaps:\")\n",
    "print(f\"Total Loss:  {total_transfer_gap:>+6.1f}%\")\n",
    "print(f\"State Loss:  {state_transfer_gap:>+6.1f}%\")\n",
    "print(f\"Action Loss: {action_transfer_gap:>+6.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
