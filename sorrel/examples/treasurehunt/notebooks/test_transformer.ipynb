{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model + RPG testing pipeline\n",
    "\n",
    "This document lays out the procedure for:\n",
    "- Training the forward model to learn the RPG environment and saving it to disk\n",
    "- Using the trained forward model to generate expert memories and saving them\n",
    "- Loading the expert memories into the transformer model\n",
    "- Training the transformer model\n",
    "\n",
    "## Table of Contents\n",
    "1. [Train the forward model](#Train-the-forward-model)\n",
    "2. [Locate the saved model](#Locate-the-saved-model)\n",
    "3. [Generate memories and save to disk](#Generate-memories-and-save-to-disk)\n",
    "4. [Evaluate the transformer model](#Evaluate-the-transformer-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# region: Imports #\n",
    "# --------------- #\n",
    "\n",
    "# Import base packages\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Going to sorrel root\n",
    "module_path = Path().resolve().parent.parent.parent\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.insert(0, str(module_path))\n",
    "\n",
    "\n",
    "from sorrel.examples.treasurehunt.entities import EmptyEntity\n",
    "from sorrel.examples.treasurehunt.env import TreasurehuntEnv\n",
    "from sorrel.examples.treasurehunt.world import TreasurehuntWorld\n",
    "from sorrel.utils.logging import TensorboardLogger\n",
    "\n",
    "# --------------- #\n",
    "# endregion       #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the forward model\n",
    "\n",
    "To give the transformer model something to learn, we need to first train a model to solve the RPG task.\n",
    "\n",
    "To edit details of the model training regime, use the configuration file (by default, stored in `../configs/config.yaml` relative to this Python notebook) to change them. Some details need to be changed together for the model to successfully run.\n",
    "\n",
    "In addition to providing a simple console log summary of each epoch, more detailed data is stored in TensorBoard if `log` in the configuration file is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = {\n",
    "    \"experiment\": {\n",
    "        \"epochs\": 1000,\n",
    "        \"max_turns\": 100,\n",
    "        \"record_period\": 50,\n",
    "        \"log_dir\": Path(\"../data/logs\") / datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"agent_vision_radius\": 2,\n",
    "        \"epsilon_decay\": 0.0005,\n",
    "    },\n",
    "    \"world\": {\n",
    "        \"height\": 20,\n",
    "        \"width\": 20,\n",
    "        \"gem_value\": 10,\n",
    "        \"food_value\": 5,\n",
    "        \"bone_value\": -10,\n",
    "        \"spawn_prob\": 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "# construct the world\n",
    "world = TreasurehuntWorld(config=config, default_entity=EmptyEntity())\n",
    "\n",
    "# construct the environment\n",
    "env = TreasurehuntEnv(world, config)\n",
    "\n",
    "# model save path\n",
    "model_path = (\n",
    "    Path(\"../models/checkpoints\")\n",
    "    / f\"treasurehunt_model_{datetime.now().strftime('%Y%m%d-%H%M%S')}.pkl\"\n",
    ")\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# run the experiment with default parameters\n",
    "env.run_experiment(\n",
    "    output_dir=Path(\"../data\"),\n",
    "    logger=TensorboardLogger.from_config(config),\n",
    ")\n",
    "\n",
    "# save model\n",
    "print(\"\\nSaving model...\")\n",
    "for agent in env.agents:\n",
    "    agent.model.save(file_path=str(model_path))\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate memories and save to disk\n",
    "\n",
    "Generating memories of game trajectories, replays of games, and model scores on games is achieved using the `eval_model()` function. This function plays a specified number of games (by default, 1) and returns a dictionary of output variables. The memories file is somewhat large (~ 1.5 GB for 1024 games), so keep that in mind.\n",
    "\n",
    "Flags that can be used:\n",
    "- `'memories'` returns a stored memory buffer of size (n_games x max_turns)\n",
    "- `'frames'` returns a list of size (n_games x max_turns) with images of each turn.\n",
    "- `'scores` returns a record of the model's reward on each turn.\n",
    "- `'jupyter-mode'` should be added when you are using `'frames'` from a Python notebook rather than from the command line.\n",
    "\n",
    "**NOTE**: As generating the frames takes a while, it's generally faster to generate animated replays and memories separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ../models/checkpoints/treasurehunt_model_20251204-140020.pkl\n",
      "\n",
      "Memories saved!\n",
      "Agent 0: ../data/memories/agent0.npz\n",
      "Agent 1: ../data/memories/agent1.npz\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(\"../models/checkpoints/treasurehunt_model_20251204-140020.pkl\")\n",
    "print(f\"Using model: {model_path}\")\n",
    "\n",
    "config = {\n",
    "    \"experiment\": {\n",
    "        \"epochs\": 1000,\n",
    "        \"max_turns\": 100,\n",
    "        \"record_period\": 50,\n",
    "        \"log_dir\": Path(\"../data/logs\") / datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"agent_vision_radius\": 2,\n",
    "        \"epsilon_decay\": 0.0005,\n",
    "    },\n",
    "    \"world\": {\n",
    "        \"height\": 20,\n",
    "        \"width\": 20,\n",
    "        \"gem_value\": 10,\n",
    "        \"food_value\": 5,\n",
    "        \"bone_value\": -10,\n",
    "        \"spawn_prob\": 0.01,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Reconstructing the same enviornment\n",
    "world = TreasurehuntWorld(config=config, default_entity=EmptyEntity())\n",
    "env = TreasurehuntEnv(world, config)\n",
    "\n",
    "# Loading saved weights\n",
    "for i, agent in enumerate(env.agents):\n",
    "    agent.model.load(file_path=str(model_path))\n",
    "\n",
    "# Generating memories\n",
    "output_dir = Path(\"../data\")\n",
    "num_games = 1024\n",
    "\n",
    "env.generate_memories(num_games=num_games, animate=False, output_dir=output_dir)\n",
    "\n",
    "\n",
    "print(f\"\\nMemories saved!\")\n",
    "print(f\"Agent 0: {output_dir / 'memories/agent0.npz'}\")\n",
    "print(f\"Agent 1: {output_dir / 'memories/agent1.npz'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the transformer model\n",
    "\n",
    "The transformer model uses a separate configuration file (by default, `../configs/transformer.yaml`). This file is shorter and handles only the details of the transformer model itself, since all of the details of the environment were already established when training the forward model. Make sure that the input parameters are compatible with those used by the forward model.\n",
    "\n",
    "**NOTE**: Be aware that at this stage, some specific configurations are not compatible with the transformer model. For example, the transformer model requires the state space to be evenly divisible by a patch size. Since the agent vision results in an odd-numbered state H x W, the state size in the forward model must have a H x W of e.g., 9 x 9, 15 x 15, 21 x 21 in order to have a patch size of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99. Total training duration: 35.427072048187256.\n"
     ]
    }
   ],
   "source": [
    "from examples.RPG.test import train_transformer_model\n",
    "\n",
    "# Load configuration path\n",
    "transformer_config_path = \"../configs/transformer.yaml\"\n",
    "cfg = load_config(argparse.Namespace(config=transformer_config_path))\n",
    "\n",
    "# Train the transformer model\n",
    "train_transformer_model(cfg, memories_path=memories_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Memories for second agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.RPG.test import eval_model\n",
    "from gem.utils import animate\n",
    "\n",
    "model_path = \"../models/checkpoints/iRainbowModel_20251018-19101760831698.pkl\"\n",
    "\n",
    "config_path = \"../configs/config.yaml\"\n",
    "\n",
    "os.makedirs(\"../data/\", exist_ok=True)\n",
    "\n",
    "results_agent_2 = eval_model(\n",
    "    \"memories\",\n",
    "    model_path=model_path,\n",
    "    config_path=config_path,\n",
    "    n_games=1024,  # Using less for testing\n",
    "    # Seed is already randomized\n",
    ")\n",
    "\n",
    "# Save the stored memories\n",
    "memories_agent_2 = results_agent_2[\"memories\"]\n",
    "memories_path_agent_2 = \"../data/memories_agent_2.pkl\"\n",
    "memories_agent_2.save(file_path=memories_path_agent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple test to validate weight transfer hypothesis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on cpu\n",
      "TEST 1: SELF PREDICTION\n",
      "\n",
      "\n",
      "Training for 10 steps:\n",
      "  Step   0: State=0.8054, Action=1.8242, Total=2.6296\n",
      "  Step   1: State=0.7942, Action=1.6624, Total=2.4567\n",
      "  Step   2: State=0.7835, Action=1.5185, Total=2.3020\n",
      "  Step   3: State=0.7736, Action=1.3947, Total=2.1683\n",
      "  Step   4: State=0.7646, Action=1.2920, Total=2.0566\n",
      "  Step   5: State=0.7562, Action=1.2101, Total=1.9663\n",
      "  Step   6: State=0.7485, Action=1.1469, Total=1.8954\n",
      "  Step   7: State=0.7413, Action=1.0996, Total=1.8409\n",
      "  Step   8: State=0.7344, Action=1.0647, Total=1.7991\n",
      "  Step   9: State=0.7277, Action=1.0389, Total=1.7667\n",
      "\n",
      "DEBUG INFO:\n",
      "Batch size: 8\n",
      "Memory size: 1024\n",
      "Can sample: True\n",
      "\n",
      "Evaluating on Agent 1:\n",
      "Agent 1 Evaluation Loss: 1.7008\n",
      "TEST 2: CROSS AGENT TRANSFER\n",
      "\n",
      "\n",
      "Evaluating on Agent 2:\n",
      "Agent 2 Evaluation Loss: 3.8146\n",
      "RESULTS\n",
      "\n",
      "\n",
      "Loss Comparison:\n",
      "Agent 1 (trained on): 1.7008\n",
      "Agent 2 (transfer to): 3.8146\n",
      "\n",
      "Transfer Gap: +124.3%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from examples.RPG.main import load_config\n",
    "from gem.models.transformer import VisionTransformer\n",
    "from gem.models.buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "\n",
    "# Load configs\n",
    "transformer_config_path = \"../configs/transformer.yaml\"\n",
    "cfg = load_config(argparse.Namespace(config=transformer_config_path))\n",
    "\n",
    "# Loading memories for both agents\n",
    "buffer_agent_1 = ReplayBuffer.load(\"../data/memories.pkl\")\n",
    "buffer_agent_2 = ReplayBuffer.load(\"../data/memories_agent_2.pkl\")\n",
    "\n",
    "# Creating model\n",
    "\n",
    "model = VisionTransformer(\n",
    "    state_size=cfg.model.state_size,\n",
    "    action_space=cfg.model.action_space,\n",
    "    layer_size=cfg.model.layer_size,\n",
    "    patch_size=cfg.model.patch_size,\n",
    "    num_frames=cfg.model.num_frames,\n",
    "    batch_size=cfg.model.batch_size,\n",
    "    num_layers=cfg.model.num_layers,\n",
    "    num_heads=cfg.model.num_heads,\n",
    "    memory=buffer_agent_1,  # Start with agent 1\n",
    "    LR=cfg.model.LR,\n",
    "    device=cfg.model.device,\n",
    "    seed=cfg.model.seed,\n",
    ")\n",
    "\n",
    "print(f\"Model created on {cfg.model.device}\")\n",
    "\n",
    "# TEST 1: Self-Prediction\n",
    "\n",
    "print(\"TEST 1: SELF PREDICTION\\n\")\n",
    "\n",
    "model.memory = buffer_agent_1\n",
    "train_steps = 10  # 500\n",
    "\n",
    "print(f\"\\nTraining for {train_steps} steps:\")\n",
    "train_losses = []\n",
    "for step in range(train_steps):\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    train_losses.append((state_loss, action_loss))\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        total = state_loss + action_loss\n",
    "        print(\n",
    "            f\"  Step {step:3d}: State={state_loss:.4f}, Action={action_loss:.4f}, Total={total:.4f}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nDEBUG INFO:\")\n",
    "print(f\"Batch size: {model.batch_size}\")\n",
    "print(f\"Memory size: {len(model.memory)}\")\n",
    "print(f\"Can sample: {len(model.memory) >= model.batch_size}\")\n",
    "\n",
    "# Evaluating on Agent 1 (self)\n",
    "print(\"\\nEvaluating on Agent 1:\")\n",
    "model.eval()\n",
    "eval_losses_agent_1 = []\n",
    "for _ in range(5):  # 20 evaluation batches\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    eval_losses_agent_1.append(state_loss + action_loss)\n",
    "model.train()\n",
    "\n",
    "avg_loss_agent_1 = np.mean(eval_losses_agent_1)\n",
    "print(f\"Agent 1 Evaluation Loss: {avg_loss_agent_1:.4f}\")\n",
    "\n",
    "# TEST 2: Cross-Agent Transfer (Evaluating on agent 2)\n",
    "\n",
    "print(\"TEST 2: CROSS AGENT TRANSFER\\n\")\n",
    "\n",
    "model.memory = buffer_agent_2\n",
    "\n",
    "print(\"\\nEvaluating on Agent 2:\")\n",
    "model.eval()\n",
    "eval_losses_agent_2 = []\n",
    "for _ in range(5):  # 20 evaluation batches\n",
    "    state_loss, action_loss = model.train_model()\n",
    "    eval_losses_agent_2.append(state_loss + action_loss)\n",
    "model.train()\n",
    "\n",
    "avg_loss_agent_2 = np.mean(eval_losses_agent_2)\n",
    "print(f\"Agent 2 Evaluation Loss: {avg_loss_agent_2:.4f}\")\n",
    "\n",
    "\n",
    "# RESULTS\n",
    "\n",
    "print(\"RESULTS\\n\")\n",
    "\n",
    "print(f\"\\nLoss Comparison:\")\n",
    "print(f\"Agent 1 (trained on): {avg_loss_agent_1:.4f}\")\n",
    "print(f\"Agent 2 (transfer to): {avg_loss_agent_2:.4f}\")\n",
    "\n",
    "transfer_gap = (avg_loss_agent_2 - avg_loss_agent_1) / avg_loss_agent_1 * 100\n",
    "print(f\"\\nTransfer Gap: {transfer_gap:+.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
