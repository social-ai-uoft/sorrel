# Brainstorming: Why Gradients Are Zero in Multiprocessing Training

## Current Situation

**Key Facts:**
1. ‚úÖ Computation graph IS connected (all tensors have `requires_grad=True` and `grad_fn`)
2. ‚úÖ Gradients are computed (not `None`) but are numerically zero
3. ‚úÖ Model CAN compute gradients with fresh random input (test shows non-zero gradients)
4. ‚ùå During actual training with batch data, gradients are zero
5. ‚úÖ Sequential version works fine (same code, no multiprocessing)
6. ‚ùå Even `huber_l.mean().backward()` produces zero gradients

## Potential Root Causes

### 1. **Shared Memory Parameter Object Identity Issue** üî¥ HIGH PRIORITY

**Hypothesis**: When we copy weights from shared memory model, the parameter objects in the computation graph might not be the same as the parameter objects in the optimizer.

**Evidence**:
- Test with fresh input works (model created fresh, no copying)
- Training with copied weights fails (weights copied from shared model)
- Optimizer parameters match model parameters by ID, but gradients are still zero

**Possible Mechanisms**:
- `load_state_dict()` or manual copying might create new parameter objects
- Computation graph might be connected to old parameter objects (from before copying)
- PyTorch might cache parameter objects in the computation graph

**Test**: Check if parameter objects in computation graph match model parameters

### 2. **PyTorch Multiprocessing Gradient Computation Bug** üî¥ HIGH PRIORITY

**Hypothesis**: There's a known or unknown bug in PyTorch's multiprocessing that prevents gradients from flowing even when the computation graph is connected.

**Evidence**:
- Everything looks correct (graph connected, parameters match, etc.)
- But gradients are still zero
- This is a known limitation: shared memory tensors can't receive gradients
- But we're using local copies, so this shouldn't apply

**Possible Mechanisms**:
- Internal PyTorch state might still reference shared memory
- Gradient computation engine might check for shared memory and skip computation
- There might be a bug in how PyTorch handles gradients in multiprocessing contexts

**Test**: Try avoiding `share_memory()` entirely and use a different IPC mechanism

### 3. **Batch Data from Shared Memory Breaking Graph** üü† MEDIUM PRIORITY

**Hypothesis**: The batch data from the shared buffer might be breaking the computation graph connection.

**Evidence**:
- Test with fresh random input works
- Training with batch data fails
- We've tried copying numpy arrays and cloning tensors, but issue persists

**Possible Mechanisms**:
- Shared memory arrays might have some internal state that breaks gradients
- Converting from numpy to tensor might preserve some shared memory connection
- The batch data might be in a format that PyTorch can't track gradients through

**Test**: Try creating completely fresh random batch data (not from shared buffer)

### 4. **Model Creation/Copying Breaking Parameter Identity** üü† MEDIUM PRIORITY

**Hypothesis**: The way we create and copy the model might be breaking the connection between parameters and the computation graph.

**Evidence**:
- We create model fresh, then copy weights
- Test with fresh model (no copying) works
- Training with copied weights fails

**Possible Mechanisms**:
- `load_state_dict()` might replace parameter objects
- Manual copying with `.data.copy_()` might not preserve all internal state
- The model might have internal references that break when copied

**Test**: Try training on the fresh model (without copying weights) to see if it works

### 5. **NoisyLinear Layer Special Behavior** üü° LOW-MEDIUM PRIORITY

**Hypothesis**: The IQN model uses `NoisyLinear` layers which might have special behavior in multiprocessing that breaks gradients.

**Evidence**:
- IQN uses NoisyLinear layers (which have learnable noise parameters)
- These layers might have special gradient computation
- Multiprocessing might interfere with noise parameter gradients

**Test**: Try a simpler model without NoisyLinear to see if issue persists

### 6. **Optimizer State Mismatch** üü° LOW PRIORITY

**Hypothesis**: The optimizer might be connected to wrong parameter objects or have wrong internal state.

**Evidence**:
- We recreate optimizer after copying weights
- Optimizer parameters match model parameters by ID
- But gradients are still zero

**Possible Mechanisms**:
- Optimizer might have cached references to old parameter objects
- Optimizer state might be corrupted
- The way we recreate optimizer might not work correctly

**Test**: Try creating optimizer before copying weights, or try different optimizer

### 7. **Loss Computation Detaching from Graph** üü° LOW PRIORITY

**Hypothesis**: Something in the loss computation might be detaching from the computation graph.

**Evidence**:
- All intermediate tensors have `requires_grad=True`
- But gradients are still zero
- Even `huber_l.mean().backward()` produces zero gradients

**Possible Mechanisms**:
- The quantile computation might be detaching somehow
- The `abs()` or other operations might break gradients
- The multiplication with quantile weights might zero out gradients

**Test**: Try using just `huber_l.mean()` as loss (without quantile computation)

### 8. **Device/Context Issues** üü¢ LOW PRIORITY

**Hypothesis**: There might be device or context issues that break gradients.

**Evidence**:
- We're using CPU for training
- Test works on same device
- But training fails

**Possible Mechanisms**:
- Multiprocessing might create device context issues
- CPU device might have special behavior in multiprocessing
- Context switching might break gradients

**Test**: Try different devices or check device context

## Recommended Investigation Order

1. **Test with fresh model (no weight copying)**: Create model fresh and train without copying weights from shared model. If this works, issue is with copying.

2. **Test with random batch data**: Instead of using batch from shared buffer, create random batch data. If this works, issue is with shared buffer data.

3. **Test without share_memory()**: Try avoiding `share_memory()` entirely and use queues or other IPC. If this works, issue is with shared memory.

4. **Test simpler model**: Try a simpler model without NoisyLinear. If this works, issue is with NoisyLinear.

5. **Test loss computation**: Try using just `huber_l.mean()` as loss. If this works, issue is with quantile computation.

## Potential Solutions

### Solution 1: Avoid Shared Memory Entirely
- Use queues to send model weights between processes
- Use shared arrays (numpy) and manually copy weights
- Use a different multiprocessing strategy (e.g., Ray, multiprocessing with explicit synchronization)

### Solution 2: Fix Parameter Object Identity
- Ensure parameter objects are preserved when copying
- Recreate optimizer with correct parameter references
- Verify computation graph uses same parameter objects

### Solution 3: Use Different Model Copying Strategy
- Serialize/deserialize model completely (break all connections)
- Use `torch.jit.script()` to create independent model
- Create model fresh and only copy weights (not entire model)

### Solution 4: Workaround - Train Without Multiprocessing
- Use threading instead of multiprocessing
- Use single process with async training
- Accept slower training but working gradients

## Next Steps

1. Add test to check if parameter objects in computation graph match model parameters
2. Test training with fresh model (no weight copying)
3. Test training with random batch data (not from shared buffer)
4. Research PyTorch multiprocessing gradient computation issues
5. Consider alternative multiprocessing strategies

