{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "from tensorflow.python.framework.errors_impl import DataLossError\n",
    "from collections import defaultdict\n",
    "import os, csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Convert tensorboard event files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_to_csv(event_file, csv_file):\n",
    "    \"\"\"\n",
    "    Convert TensorBoard event file data to a CSV format.\n",
    "\n",
    "    Args:\n",
    "        event_file (str): Path to the TensorBoard event file (e.g., events.out.tfevents.xxx).\n",
    "        csv_file (str): Path where the output CSV file should be saved.\n",
    "    \"\"\"\n",
    "    # Create a list to store the data rows\n",
    "    data_rows = []\n",
    "    \n",
    "    # Use tf.compat.v1 to access the summary_iterator in TensorFlow 2.x\n",
    "    for e in tf.compat.v1.train.summary_iterator(event_file):\n",
    "        for v in e.summary.value:\n",
    "            # Only consider scalar summaries\n",
    "            if v.HasField('simple_value'):\n",
    "                tag = v.tag\n",
    "                value = v.simple_value\n",
    "                step = e.step\n",
    "                data_rows.append([step, tag, value])\n",
    "    \n",
    "    # Write the extracted data into a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Step', 'Tag', 'Value'])\n",
    "        # Write the data rows\n",
    "        writer.writerows(data_rows)\n",
    "    \n",
    "    print(f\"Data from {event_file} has been written to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_to_separate_csv(event_file, output_dir):\n",
    "    \"\"\"\n",
    "    Convert TensorBoard event file data to separate CSV files for each tag.\n",
    "    \"\"\"\n",
    "    tag_data = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        for e in tf.compat.v1.train.summary_iterator(event_file):\n",
    "            try:\n",
    "                # Process individual records\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        tag = v.tag\n",
    "                        value = v.simple_value\n",
    "                        step = e.step\n",
    "                        tag_data[tag].append([step, value])\n",
    "            except Exception as record_error:\n",
    "                # Log a warning and skip problematic records\n",
    "                print(f\"Skipped a corrupt record in file: {event_file}\")\n",
    "    except DataLossError:\n",
    "        # Log and continue for files with partial writes\n",
    "        print(f\"Encountered DataLossError. Possibly due to incomplete writes in file: {event_file}\")\n",
    "\n",
    "    # Save tag data to CSV files\n",
    "    for tag, data_rows in tag_data.items():\n",
    "        filename = f\"{output_dir}/{tag.replace('/', '_')}_data.csv\"\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Step', 'Value'])\n",
    "            writer.writerows(data_rows)\n",
    "        print(f\"Data for tag '{tag}' has been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_stable(file_path, wait_time=1.0):\n",
    "    initial_size = os.path.getsize(file_path)\n",
    "    time.sleep(wait_time)\n",
    "    final_size = os.path.getsize(file_path)\n",
    "    return initial_size == final_size\n",
    "\n",
    "def process_tensorboard_results(parent_dir, output_parent_dir):\n",
    "    \"\"\"\n",
    "    Loop through all subdirectories of a parent directory, process TensorBoard files,\n",
    "    and save results to a corresponding directory structure.\n",
    "\n",
    "    Args:\n",
    "        parent_dir (str): Parent directory containing TensorBoard event files.\n",
    "        output_parent_dir (str): Parent directory where CSV files should be saved.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if \"tfevents\" in file:  # Check if the file is a TensorBoard event file\n",
    "                event_file = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, parent_dir)\n",
    "                output_dir = os.path.join(output_parent_dir, relative_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                # Skip processing if output file already exists\n",
    "                if os.path.exists(output_dir):\n",
    "                    print(f\"Skipping already processed file: {event_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Check if the file is stable\n",
    "                if not is_file_stable(event_file):\n",
    "                    print(f\"File is still being written: {event_file}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    tensorboard_to_separate_csv(event_file, output_dir)\n",
    "                    print(f\"Processed {event_file} -> {output_dir}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {event_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: Plotting and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_moving_average(data, alpha):\n",
    "    \"\"\"\n",
    "    Calculate the exponential moving average (EMA) of a 1D array.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data.\n",
    "        alpha (float): The smoothing factor (0 < alpha <= 1).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The EMA values.\n",
    "    \"\"\"\n",
    "    if not (0 < alpha <= 1):\n",
    "        raise ValueError(\"Alpha must be between 0 and 1.\")\n",
    "\n",
    "    ema = [data[0]]  # Initialize EMA with the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        ema.append(alpha * data[i] + (1 - alpha) * ema[-1])\n",
    "    return np.array(ema)\n",
    "\n",
    "\n",
    "def rolling_average(data, window_size):\n",
    "    \"\"\"\n",
    "    Calculate the rolling average of a 1D array.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data.\n",
    "        window_size (int): The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rolling average values.\n",
    "    \"\"\"\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1.\")\n",
    "    if len(data) < window_size:\n",
    "        raise ValueError(\"Data length must be at least equal to the window size.\")\n",
    "    \n",
    "    # Use np.convolve for efficient computation\n",
    "    weights = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, weights, mode='valid')\n",
    "\n",
    "def plot_average_trajectory(time_series, error_type='std', time_points=None, xlabel='Time', ylabel='Value', title='Average Trajectory'):\n",
    "    \"\"\"\n",
    "    Plots the average trajectory of a set of time series with error bars.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series (2D array-like): A set of time series, shape (n_series, n_time_points).\n",
    "    - error_type (str): Either 'std' for standard deviation or 'sem' for standard error.\n",
    "    - time_points (1D array-like, optional): Time points corresponding to the time series. Defaults to indices.\n",
    "    - xlabel (str): Label for the x-axis.\n",
    "    - ylabel (str): Label for the y-axis.\n",
    "    - title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    time_series = np.array(time_series)\n",
    "    if time_points is None:\n",
    "        time_points = np.arange(time_series.shape[1])\n",
    "    else:\n",
    "        time_points = np.array(time_points)\n",
    "    \n",
    "    if time_series.shape[1] != len(time_points):\n",
    "        raise ValueError(\"Length of time_points must match the number of columns in time_series.\")\n",
    "    \n",
    "    # Compute average and error\n",
    "    mean_trajectory = np.mean(time_series, axis=0)\n",
    "    if error_type == 'std':\n",
    "        error = np.std(time_series, axis=0)\n",
    "    elif error_type == 'sem':\n",
    "        error = np.std(time_series, axis=0) / np.sqrt(time_series.shape[0])\n",
    "    else:\n",
    "        raise ValueError(\"error_type must be 'std' or 'sem'.\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_points, mean_trajectory, label='Mean Trajectory', color='blue')\n",
    "    plt.fill_between(time_points, mean_trajectory - error, mean_trajectory + error, alpha=0.3, color='blue', label='Error')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def trim_and_calculate_mean(array_list):\n",
    "    # Find the minimum length among all arrays\n",
    "    min_length = min(len(arr) for arr in array_list)\n",
    "    \n",
    "    # Trim each array to the minimum length\n",
    "    trimmed_arrays = [arr[:min_length] for arr in array_list]\n",
    "    \n",
    "    # Convert to a NumPy array for vectorized mean calculation\n",
    "    trimmed_arrays = np.array(trimmed_arrays)\n",
    "    \n",
    "    return trimmed_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "def compare_binned_time_series(data_group1, data_group2, num_bins=10, test_func=ttest_ind, **test_kwargs):\n",
    "    \"\"\"\n",
    "    Compare two groups' time-series data by dividing the time course into bins,\n",
    "    averaging values in each bin, and performing a specified statistical test.\n",
    "\n",
    "    Args:\n",
    "        data_group1 (list of lists or np.array): Time-series data for group 1 (each row is an instance).\n",
    "        data_group2 (list of lists or np.array): Time-series data for group 2 (each row is an instance).\n",
    "        num_bins (int): Number of bins to divide the time course.\n",
    "        test_func (callable): Statistical test function (default: t-test). \n",
    "                              Must accept two arrays and return a statistic and p-value.\n",
    "        **test_kwargs: Additional keyword arguments to pass to the statistical test.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains test statistics, p-values, and Bonferroni-corrected significance for each bin.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays\n",
    "    data_group1 = np.array(data_group1)\n",
    "    data_group2 = np.array(data_group2)\n",
    "\n",
    "    # Ensure both groups have the same number of time steps\n",
    "    assert data_group1.shape[1] == data_group2.shape[1], \"Mismatched time steps\"\n",
    "\n",
    "    # Define bin edges\n",
    "    num_timesteps = data_group1.shape[1]\n",
    "    bin_edges = np.linspace(0, num_timesteps, num_bins + 1, dtype=int)\n",
    "\n",
    "    results = {}\n",
    "    alpha_corrected = 0.05 / num_bins  # Bonferroni correction\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        start, end = bin_edges[i], bin_edges[i+1]\n",
    "\n",
    "        # Compute mean values in the bin for each instance\n",
    "        mean_group1 = data_group1[:, start:end].mean(axis=1)\n",
    "        mean_group2 = data_group2[:, start:end].mean(axis=1)\n",
    "\n",
    "        # Perform the specified test\n",
    "        test_stat, p_val = test_func(mean_group1, mean_group2, **test_kwargs)\n",
    "\n",
    "        # Store results\n",
    "        results[f\"Bin {i+1}\"] = {\n",
    "            \"test_statistic\": test_stat,\n",
    "            \"p-value\": p_val,\n",
    "            \"threshold after Bonferroni correction\": alpha_corrected,\n",
    "            \"significant (Bonferroni)\": p_val < alpha_corrected\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_group_data_with_specified_tag(folder, tag_keyword, has_child_dir=False):\n",
    "    \"\"\"\n",
    "    Read group data from CSV files with a specified tag in the given folder.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The folder containing the CSV files.\n",
    "        tag (str): The tag to search for in the CSV file names.\n",
    "        has_child_dir (bool): Whether the CSV files are in subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the data read from the CSV files.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.join('res', folder)\n",
    "    files = os.listdir(parent_dir)\n",
    "\n",
    "    group_data = []\n",
    "\n",
    "    data = None \n",
    "\n",
    "    if has_child_dir:\n",
    "        for file in files:\n",
    "            if tag_keyword in file:\n",
    "                sub_dir = os.path.join(parent_dir, file)            \n",
    "                file = os.listdir(sub_dir)[0]\n",
    "                data = pd.read_csv(os.path.join(sub_dir, file))\n",
    "                group_data.append(data['Value'].to_numpy())\n",
    "    else:\n",
    "        for file in files:\n",
    "            if tag_keyword in file:\n",
    "                data = pd.read_csv(os.path.join(parent_dir, file))\n",
    "                group_data.append(data['Value'].to_numpy())\n",
    "\n",
    "    assert data is not None, f\"No file with tag '{tag_keyword}' found in folder '{folder}'.\"\n",
    "    \n",
    "    return group_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_per_condition(folders, wsize, tag_keyword, labels, has_child_dir):\n",
    "    \"\"\"\n",
    "    Plot the data for each condition.\n",
    "    \"\"\"\n",
    "    collective_d = [[] for _ in range(len(folders))]\n",
    "    for ixs, folder in enumerate(folders):\n",
    "        collective_d[ixs] = read_group_data_with_specified_tag(folder, tag_keyword, has_child_dir)\n",
    "    collective_d_avg = [np.mean(trim_and_calculate_mean(collective_d[i]), axis=0) for i in range(len(collective_d))]\n",
    "\n",
    "    # plot\n",
    "    # alpha = 0.99\n",
    "    wsize = wsize\n",
    "    for ixs, d in enumerate(collective_d_avg):\n",
    "        # plt.plot(exponential_moving_average(d, alpha), label=labels[ixs])\n",
    "        plt.plot(rolling_average(d, wsize), label=labels[ixs])\n",
    "    plt.legend()\n",
    "    plt.title(f'Mean {tag_keyword}')\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(labels), figsize=(4*len(labels),4))\n",
    "    wsize = wsize\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(collective_d[i])):\n",
    "            axes[i].plot(rolling_average(collective_d[i][j], wsize))\n",
    "        axes[i].set_title(labels[i])\n",
    "    plt.suptitle(f'Individual {tag_keyword}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
