experiment:
  epochs: 100

model:
  state_size: [3, 15, 15]
  action_space: 4
  patch_size: 3
  layer_size: 200
  num_frames: 99 # There are 100 timesteps, but only 99 transitions since we have to start from the 2nd timestep
  num_heads: 4 # Number of attention heads
  batch_size: 64
  num_layers: 4 # Number of transformer blocks
  LR: .0001
  device: mps

# Root folder for accessing file structure and modules
root: '/Users/rgelpi/Documents/GitHub/transformers'
log: False