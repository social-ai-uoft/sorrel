{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# --------------- #\n",
    "# region: Imports #\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('../../..')\n",
    "if module_path not in sys.path:\n",
    "  sys.path.insert(0, module_path)\n",
    "# endregion   #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.cleanup.env import Cleanup\n",
    "from examples.cleanup.agents import Agent\n",
    "from examples.RPG.utils import load_config\n",
    "from gem.models.grid_cells import positional_embedding\n",
    "from gem.models.DDQN import doubleDQN\n",
    "from gem.models.iqn import iRainbowModel\n",
    "from gem.utils import visual_field_sprite, image_from_array, animate, one_hot_encode\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "cfg = load_config(argparse.Namespace(config='../configs/config.yaml'))\n",
    "\n",
    "seed = random.randint(1,100)\n",
    "\n",
    "N_AGENTS = 1\n",
    "agents = []\n",
    "for i in range(N_AGENTS):\n",
    "  agents.append(\n",
    "  Agent(cfg, appearance = cfg.agent.agent.appearance, \n",
    "    #     model = doubleDQN(\n",
    "    #     input_size=5224,\n",
    "    #     number_of_actions=4,\n",
    "    #     lr=0.001,\n",
    "    #     gamma=0.97,\n",
    "    #     per=False,\n",
    "    #     alpha=0.6,\n",
    "    #     beta=0.05,\n",
    "    #     beta_increment=0.0006,\n",
    "    #     capacity=5000,\n",
    "    # )\n",
    "    model = iRainbowModel(\n",
    "      state_size=[8,9,9],\n",
    "      action_size=4,\n",
    "      layer_size=250,\n",
    "      epsilon=.9,\n",
    "      device=\"mps\",\n",
    "      seed=seed,\n",
    "      num_frames=5,\n",
    "      n_step=3,\n",
    "      BATCH_SIZE= 64,\n",
    "      BUFFER_SIZE= 1024,\n",
    "      LR=0.001,\n",
    "      TAU=.001,\n",
    "      GAMMA=0.99,\n",
    "      N=12,\n",
    "      sync_freq=200,\n",
    "      model_update_freq=4\n",
    "    )\n",
    "          )\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = Cleanup(\n",
    "  cfg, agents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Epsilon: 0.8991 - Losses 0 - Avg. last 100 rewards: 5.0\n",
      "Epoch: 25 - Epsilon: 0.8768901733960046 - Losses 0.2443152368068695 - Avg. last 100 rewards: 4.3076923076923075\n",
      "Epoch: 50 - Epsilon: 0.8552289803119505 - Losses 0.7499598264694214 - Avg. last 100 rewards: 4.0588235294117645\n",
      "Epoch: 75 - Epsilon: 0.8341028682449498 - Losses 0.06996339559555054 - Avg. last 100 rewards: 3.9342105263157894\n",
      "Epoch: 100 - Epsilon: 0.8134986194699355 - Losses 0.2628120183944702 - Avg. last 100 rewards: 3.87\n",
      "Epoch: 125 - Epsilon: 0.7934033427698834 - Losses 0.32234248518943787 - Avg. last 100 rewards: 3.99\n",
      "Epoch: 150 - Epsilon: 0.7738044653703178 - Losses 0.34577804803848267 - Avg. last 100 rewards: 3.91\n",
      "Epoch: 175 - Epsilon: 0.7546897250730513 - Losses 0.2992754876613617 - Avg. last 100 rewards: 3.99\n",
      "Epoch: 200 - Epsilon: 0.7360471625842407 - Losses 0.8197137117385864 - Avg. last 100 rewards: 4.21\n",
      "Epoch: 225 - Epsilon: 0.7178651140319564 - Losses 0.4004423916339874 - Avg. last 100 rewards: 3.83\n",
      "Epoch: 250 - Epsilon: 0.7001322036685851 - Losses 0.2712719440460205 - Avg. last 100 rewards: 4.03\n",
      "Epoch: 275 - Epsilon: 0.6828373367535008 - Losses 0.5596045255661011 - Avg. last 100 rewards: 4.12\n",
      "Epoch: 300 - Epsilon: 0.6659696926115485 - Losses 0.24554017186164856 - Avg. last 100 rewards: 4.03\n",
      "Epoch: 325 - Epsilon: 0.6495187178630015 - Losses 0.5235594511032104 - Avg. last 100 rewards: 4.28\n",
      "Epoch: 350 - Epsilon: 0.6334741198207515 - Losses 0.6235806941986084 - Avg. last 100 rewards: 4.1\n",
      "Epoch: 375 - Epsilon: 0.6178258600506062 - Losses 0.39476966857910156 - Avg. last 100 rewards: 3.78\n",
      "Epoch: 400 - Epsilon: 0.6025641480906593 - Losses 0.5956558585166931 - Avg. last 100 rewards: 3.54\n",
      "Epoch: 425 - Epsilon: 0.587679435325808 - Losses 0.5665527582168579 - Avg. last 100 rewards: 3.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m         agent\u001b[38;5;241m.\u001b[39mepisode_memory\u001b[38;5;241m.\u001b[39mappend(exp)\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m#TODO: decide on memory update procedures\u001b[39;00m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m# agent.model.replay_buffer.add(torch.tensor(state), action, reward, torch.tensor(next_state), done)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_epoch_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         game_points \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     76\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(game_points)\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/gem/models/iqn.py:397\u001b[0m, in \u001b[0;36miRainbowModel.end_epoch_action\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransfer_memories(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m], extra_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgame_vars\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    399\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgame_vars\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/gem/models/iqn.py:297\u001b[0m, in \u001b[0;36miRainbowModel.train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH_SIZE:\n\u001b[0;32m--> 297\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# Get max predicted Q values (for next states) from target model\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     Q_targets_next, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_target(next_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN)\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/gem/models/buffer.py:118\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m rewards \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[1;32m    104\u001b[0m         np\u001b[38;5;241m.\u001b[39mvstack([e\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m next_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[1;32m    111\u001b[0m         np\u001b[38;5;241m.\u001b[39mstack([e\u001b[38;5;241m.\u001b[39mnext_state \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m dones \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    117\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m--> 118\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    119\u001b[0m             np\u001b[38;5;241m.\u001b[39muint8\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (states, actions, rewards, next_states, dones)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg.experiment.epochs = 10000 # override the number of epochs\n",
    "\n",
    "rewards = []\n",
    "losses = 0\n",
    "epsilon = .9\n",
    "\n",
    "EPOCH_PRINT_FREQ = 25\n",
    "EPSILON_DECAY_RATE = 0.999\n",
    "EPSILON_DECAY_FREQ = 1\n",
    "EVAL_EPSILON = 0.05\n",
    "\n",
    "for epoch in range(cfg.experiment.epochs): # note that the language is not right. epoch is training. episode is the game\n",
    "    # Reset the environment at the start of each epoch\n",
    "        env.reset()\n",
    "        for agent in env.agents:\n",
    "            agent.reset()\n",
    "            agent.init_replay()\n",
    "            agent.model.start_epoch_action(**locals())\n",
    "        random.shuffle(agents)\n",
    "\n",
    "        done = 0 \n",
    "        turn = 0\n",
    "        losses = 0\n",
    "        game_points = 0\n",
    "\n",
    "        images = []\n",
    "        if epoch % EPSILON_DECAY_FREQ == 0:\n",
    "            epsilon = epsilon*EPSILON_DECAY_RATE\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            turn = turn + 1\n",
    "\n",
    "            entities = env.get_entities_for_transition()\n",
    "            # Entity transition\n",
    "            for entity in entities:\n",
    "                entity.transition(env)\n",
    "\n",
    "            # Agent transition\n",
    "            for agent in agents:\n",
    "                state = agent.pov_stack(env)\n",
    "\n",
    "                if epoch % EPOCH_PRINT_FREQ == 0:\n",
    "                    _image = visual_field_sprite(env.world)\n",
    "                    image = image_from_array(_image)\n",
    "                    images.append(image)\n",
    "\n",
    "                # Take action based on current state\n",
    "                if epoch % EPOCH_PRINT_FREQ == 0:\n",
    "                    action = agent.model.take_action(state, EVAL_EPSILON)\n",
    "                else:\n",
    "                    action = agent.model.take_action(state, epsilon)\n",
    "\n",
    "                (reward,\n",
    "                next_state,\n",
    "                done_\n",
    "                ) = agent.transition(env, state, action)\n",
    "\n",
    "                if turn >= cfg.experiment.max_turns or done_:\n",
    "                    done = 1\n",
    "\n",
    "                exp = (1, (state, action, reward, next_state, done))\n",
    "                agent.episode_memory.append(exp)\n",
    "                #TODO: decide on memory update procedures\n",
    "                agent.model.end_epoch_action(**locals())\n",
    "\n",
    "                game_points += reward\n",
    "\n",
    "        rewards.append(game_points)\n",
    "        \n",
    "        # At the end of each epoch, train as long as the batch size is large enough.\n",
    "        if epoch > 10:\n",
    "            loss = agent.model.train_model()\n",
    "            losses += loss\n",
    "            \n",
    "        # Calculate the average of the last 100 rewards\n",
    "        if len(rewards) >= 100:\n",
    "            avg_last_100_rewards = sum(rewards[-100:]) / 100\n",
    "        else:\n",
    "            avg_last_100_rewards = sum(rewards) / len(rewards)\n",
    "        if epoch % EPOCH_PRINT_FREQ == 0:\n",
    "            print(f'Epoch: {epoch} - Epsilon: {epsilon} - Losses {losses} - Avg. last 100 rewards: {avg_last_100_rewards}')\n",
    "            animate(\n",
    "                images, filename = f\"cleanup_epoch{epoch}\", folder = f\"{cfg.root}/examples/cleanup/data/\"\n",
    "            )\n",
    "            losses = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
