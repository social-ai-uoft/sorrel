{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# --------------- #\n",
    "# region: Imports #\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('../../..')\n",
    "if module_path not in sys.path:\n",
    "  sys.path.insert(0, module_path)\n",
    "# endregion   #\n",
    "# --------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.cleanup.env import Cleanup\n",
    "from examples.cleanup.agents import Agent\n",
    "from examples.RPG.utils import load_config\n",
    "from gem.models.grid_cells import positional_embedding\n",
    "from gem.models.human_player import ModelHumanPlayer\n",
    "from gem.models.iqn import iRainbowModel\n",
    "from gem.utils import visual_field, visual_field_multilayer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "cfg = load_config(argparse.Namespace(config='../configs/config.yaml'))\n",
    "\n",
    "N_AGENTS = 3\n",
    "agents = []\n",
    "for i in range(N_AGENTS):\n",
    "  agents.append(\n",
    "  Agent(cfg, appearance = cfg.agent.agent.appearance, \n",
    "        model = iRainbowModel(\n",
    "        state_size= [cfg.env.channels, cfg.env.height, cfg.env.width],\n",
    "        action_size= 6,\n",
    "        layer_size= 128,\n",
    "        epsilon= 1.0,\n",
    "        #device= Union[str, torch.device],\n",
    "        device = 'cpu',\n",
    "        seed= 0,\n",
    "        # iRainbow parameters\n",
    "        num_frames= 1,\n",
    "        n_step= 3,\n",
    "        sync_freq=10,\n",
    "        model_update_freq= 10,\n",
    "        BATCH_SIZE= 32,\n",
    "        BUFFER_SIZE= 4000,\n",
    "        LR= .001,\n",
    "        TAU= .001,\n",
    "        GAMMA= .99,\n",
    "        N= 32\n",
    "          )\n",
    "          )\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = Cleanup(\n",
    "  cfg, agents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0327, -0.3010,  0.0611,  0.0587, -0.0498, -0.1373],\n",
       "          [ 0.0340, -0.2985,  0.0629,  0.0592, -0.0479, -0.1337],\n",
       "          [ 0.0331, -0.3015,  0.0595,  0.0573, -0.0512, -0.1355],\n",
       "          [ 0.0301, -0.3012,  0.0600,  0.0550, -0.0515, -0.1361],\n",
       "          [ 0.0328, -0.2998,  0.0617,  0.0576, -0.0513, -0.1370],\n",
       "          [ 0.0302, -0.3015,  0.0602,  0.0559, -0.0516, -0.1365],\n",
       "          [ 0.0359, -0.2969,  0.0638,  0.0597, -0.0456, -0.1321],\n",
       "          [ 0.0288, -0.3012,  0.0611,  0.0574, -0.0513, -0.1345]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.0547],\n",
       "          [0.2592],\n",
       "          [0.7171],\n",
       "          [0.9299],\n",
       "          [0.1539],\n",
       "          [0.9226],\n",
       "          [0.6731],\n",
       "          [0.8101]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(cfg.env.channels, cfg.env.height, cfg.env.width).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "agents[0].model.qnetwork_local.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Points: 0\n",
      "Epoch: 1 - Points: 0\n",
      "Epoch: 2 - Points: 0\n",
      "Epoch: 3 - Points: 0\n",
      "Epoch: 4 - Points: 0\n",
      "Epoch: 5 - Points: 0\n",
      "Epoch: 6 - Points: 0\n",
      "Epoch: 7 - Points: 0\n",
      "Epoch: 8 - Points: 0\n",
      "Epoch: 9 - Points: 0\n",
      "Epoch: 10 - Points: 0\n",
      "Epoch: 11 - Points: 0\n",
      "Epoch: 12 - Points: 0\n",
      "Epoch: 13 - Points: 0\n",
      "Epoch: 14 - Points: 0\n",
      "Epoch: 15 - Points: 0\n",
      "Epoch: 16 - Points: 0\n",
      "Epoch: 17 - Points: 0\n",
      "Epoch: 18 - Points: 0\n",
      "Epoch: 19 - Points: 0\n",
      "Epoch: 20 - Points: 0\n",
      "Epoch: 21 - Points: 0\n",
      "Epoch: 22 - Points: 0\n",
      "Epoch: 23 - Points: 0\n",
      "Epoch: 24 - Points: 0\n",
      "Epoch: 25 - Points: 0\n",
      "Epoch: 26 - Points: 0\n",
      "Epoch: 27 - Points: 0\n",
      "Epoch: 28 - Points: 0\n",
      "Epoch: 29 - Points: 0\n",
      "Epoch: 30 - Points: 0\n",
      "Epoch: 31 - Points: 0\n",
      "Epoch: 32 - Points: 0\n",
      "Epoch: 33 - Points: 0\n",
      "Epoch: 34 - Points: 0\n",
      "Epoch: 35 - Points: 0\n",
      "Epoch: 36 - Points: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Agent transition\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#location_code = positional_embedding(agent.location, env, 3, 3)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     (state,\n\u001b[1;32m     32\u001b[0m     action,\n\u001b[1;32m     33\u001b[0m     reward,\n\u001b[1;32m     34\u001b[0m     next_state,\n\u001b[1;32m     35\u001b[0m     done_\n\u001b[0;32m---> 36\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m turn \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mmax_turns \u001b[38;5;129;01mor\u001b[39;00m done_:\n\u001b[1;32m     39\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/examples/cleanup/agents.py:122\u001b[0m, in \u001b[0;36mAgent.transition\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Changes the world based on action taken.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# Get current state\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpov\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Take action based on current state\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/examples/cleanup/agents.py:108\u001b[0m, in \u001b[0;36mAgent.pov\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# If the environment is a full MDP, get the whole world image\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mfull_mdp:\n\u001b[0;32m--> 108\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mvisual_field_multilayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Otherwise, use the agent observation function\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     image \u001b[38;5;241m=\u001b[39m visual_field_multilayer(env\u001b[38;5;241m.\u001b[39mworld, env\u001b[38;5;241m.\u001b[39mcolor_map, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision, env\u001b[38;5;241m.\u001b[39mchannels)\n",
      "File \u001b[0;32m~/Documents/GitHub/agentarium/gem/utils.py:166\u001b[0m, in \u001b[0;36mvisual_field_multilayer\u001b[0;34m(world, color_map, location, vision, channels, return_rgb)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m world\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m         new[:, H, W, layer] \u001b[38;5;241m=\u001b[39m world[H, W, layer]\u001b[38;5;241m.\u001b[39mappearance\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    168\u001b[0m         new[:, H, W] \u001b[38;5;241m=\u001b[39m world[H, W, layer]\u001b[38;5;241m.\u001b[39mappearance\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg.experiment.epochs = 10000 # override the number of epochs\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(cfg.experiment.epochs):\n",
    "    # Reset the environment at the start of each epoch\n",
    "        for agent in env.agents:\n",
    "            agent.reset()\n",
    "        random.shuffle(agents)\n",
    "\n",
    "        done = 0 \n",
    "        turn = 0\n",
    "        losses = 0\n",
    "        game_points = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            turn = turn + 1\n",
    "\n",
    "            entities = env.get_entities_for_transition()\n",
    "            #print(entities)\n",
    "            # Entity transition\n",
    "            for entity in entities:\n",
    "                entity.transition(env)\n",
    "\n",
    "            # Agent transition\n",
    "            for agent in agents:\n",
    "\n",
    "                #location_code = positional_embedding(agent.location, env, 3, 3)\n",
    "\n",
    "                (state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state,\n",
    "                done_\n",
    "                ) = agent.transition(env)\n",
    "\n",
    "                if turn >= cfg.experiment.max_turns or done_:\n",
    "                    done = 1\n",
    "\n",
    "                exp = (1, (state, action, reward, next_state, done))\n",
    "                agent.episode_memory.append(exp)\n",
    "\n",
    "                game_points += reward\n",
    "        rewards.append(game_points)\n",
    "        print(f'Epoch: {epoch} - Points: {game_points}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_positional_code \u001b[38;5;241m=\u001b[39m generate_positional_embedding_for_coordinate(agents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlocation[\u001b[38;5;241m0\u001b[39m], agent\u001b[38;5;241m.\u001b[39mlocation[\u001b[38;5;241m1\u001b[39m], env\u001b[38;5;241m.\u001b[39mheight, env\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12, 1)\n",
      "(11, 17, 1)\n",
      "(8, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "for agent in agents:\n",
    "    print(agent.location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
