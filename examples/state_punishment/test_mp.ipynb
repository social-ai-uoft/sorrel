{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e894fe0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'wnsm_a8559af3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    143\u001b[0m     mp\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 137\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m     locks\u001b[38;5;241m.\u001b[39mappend(lock)\n\u001b[0;32m    135\u001b[0m     learners\u001b[38;5;241m.\u001b[39mappend(learner)\n\u001b[1;32m--> 137\u001b[0m \u001b[43mactor_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learner \u001b[38;5;129;01min\u001b[39;00m learners:\n\u001b[0;32m    140\u001b[0m     learner\u001b[38;5;241m.\u001b[39mjoin()\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mactor_loop\u001b[1;34m(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactor_loop\u001b[39m(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch):\n\u001b[0;32m     70\u001b[0m     models \u001b[38;5;241m=\u001b[39m [AgentNet() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[1;32m---> 71\u001b[0m     shms \u001b[38;5;241m=\u001b[39m [shared_memory\u001b[38;5;241m.\u001b[39mSharedMemory(name\u001b[38;5;241m=\u001b[39mshared_names[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[0;32m     72\u001b[0m     weight_nps \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mndarray(shapes[i], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, buffer\u001b[38;5;241m=\u001b[39mshms[i]\u001b[38;5;241m.\u001b[39mbuf) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[0;32m     74\u001b[0m     env \u001b[38;5;241m=\u001b[39m SharedEnv()\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactor_loop\u001b[39m(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch):\n\u001b[0;32m     70\u001b[0m     models \u001b[38;5;241m=\u001b[39m [AgentNet() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[1;32m---> 71\u001b[0m     shms \u001b[38;5;241m=\u001b[39m [\u001b[43mshared_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSharedMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[0;32m     72\u001b[0m     weight_nps \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mndarray(shapes[i], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, buffer\u001b[38;5;241m=\u001b[39mshms[i]\u001b[38;5;241m.\u001b[39mbuf) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[0;32m     74\u001b[0m     env \u001b[38;5;241m=\u001b[39m SharedEnv()\n",
      "File \u001b[1;32mc:\\Users\\22455\\Anaconda3\\envs\\seeing_the_future\\lib\\multiprocessing\\shared_memory.py:161\u001b[0m, in \u001b[0;36mSharedMemory.__init__\u001b[1;34m(self, name, create, size)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Dynamically determine the existing named shared memory\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# block's size which is likely a multiple of mmap.PAGESIZE.\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m h_map \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenFileMapping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFILE_MAP_READ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     p_buf \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mMapViewOfFile(\n\u001b[0;32m    168\u001b[0m         h_map,\n\u001b[0;32m    169\u001b[0m         _winapi\u001b[38;5;241m.\u001b[39mFILE_MAP_READ,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    173\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'wnsm_a8559af3'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import shared_memory, Lock, Value\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define the agent network\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return torch.cat([p.data.view(-1) for p in self.parameters()])\n",
    "\n",
    "    def load_flat_weights(self, flat_tensor):\n",
    "        pointer = 0\n",
    "        for param in self.parameters():\n",
    "            numel = param.numel()\n",
    "            param.data.copy_(flat_tensor[pointer:pointer + numel].view_as(param))\n",
    "            pointer += numel\n",
    "\n",
    "# Shared dummy environment\n",
    "class SharedEnv:\n",
    "    def reset(self):\n",
    "        return [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "\n",
    "    def step(self, actions):\n",
    "        next_obs = [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "        rewards = [np.random.rand() for _ in range(6)]\n",
    "        dones = [np.random.rand() > 0.95 for _ in range(6)]\n",
    "        return next_obs, rewards, dones, {}\n",
    "\n",
    "# Learner process that executes exactly once per epoch\n",
    "def learner_process(agent_id, shared_name, shape, lock, shared_epoch, done_epoch):\n",
    "    shm = shared_memory.SharedMemory(name=shared_name)\n",
    "    weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "    model = AgentNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    dummy_input = torch.randn((32, 4))\n",
    "    dummy_target = torch.randint(0, 2, (32,))\n",
    "    last_epoch = 0  # No training at epoch 0\n",
    "\n",
    "    while True:\n",
    "        current_epoch = shared_epoch.value\n",
    "        if current_epoch > last_epoch:\n",
    "            output = model(dummy_input)\n",
    "            loss = nn.CrossEntropyLoss()(output, dummy_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with lock:\n",
    "                weight_np[:] = model.get_flat_weights().numpy()\n",
    "            print(f\"[Learner {agent_id}] Trained for epoch {current_epoch}\")\n",
    "\n",
    "            done_epoch[agent_id] = current_epoch\n",
    "            last_epoch = current_epoch\n",
    "        time.sleep(0.05)\n",
    "\n",
    "# Main actor loop (runs in main process)\n",
    "def actor_loop(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch):\n",
    "    models = [AgentNet() for _ in range(num_agents)]\n",
    "    shms = [shared_memory.SharedMemory(name=shared_names[i]) for i in range(num_agents)]\n",
    "    weight_nps = [np.ndarray(shapes[i], dtype=np.float32, buffer=shms[i].buf) for i in range(num_agents)]\n",
    "\n",
    "    env = SharedEnv()\n",
    "    obs_list = env.reset()\n",
    "    epoch = 0\n",
    "    steps_per_epoch = 10\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            with locks[i]:\n",
    "                weights_tensor = torch.tensor(weight_nps[i].copy(), dtype=torch.float32)\n",
    "            models[i].load_flat_weights(weights_tensor)\n",
    "            obs_tensor = torch.tensor(obs_list[i], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action = models[i](obs_tensor).argmax().item()\n",
    "            actions.append(action)\n",
    "\n",
    "        next_obs, rewards, dones, _ = env.step(actions)\n",
    "        obs_list = next_obs\n",
    "        step_count += 1\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            print(f\"[Agent {i}] Action: {actions[i]} Reward: {rewards[i]:.2f} Done: {dones[i]}\")\n",
    "\n",
    "        if step_count >= steps_per_epoch:\n",
    "            step_count = 0\n",
    "            epoch += 1\n",
    "            shared_epoch.value = epoch\n",
    "            print(f\"[Actor] Epoch {epoch} completed. Waiting for learners...\")\n",
    "\n",
    "            while not all(done_epoch[i] == epoch for i in range(num_agents)):\n",
    "                time.sleep(0.1)\n",
    "            print(f\"[Actor] All learners completed training for epoch {epoch}\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Setup function\n",
    "def main():\n",
    "    num_agents = 6\n",
    "    shared_names = []\n",
    "    shapes = []\n",
    "    locks = []\n",
    "    learners = []\n",
    "    shared_epoch = Value('i', 0)\n",
    "    done_epoch = mp.Array('i', [0] * num_agents)\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        dummy_model = AgentNet()\n",
    "        flat_weights = dummy_model.get_flat_weights()\n",
    "        shape = flat_weights.shape\n",
    "        shm = shared_memory.SharedMemory(create=True, size=flat_weights.numel() * 4)\n",
    "        weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "        weight_np[:] = flat_weights.numpy()\n",
    "\n",
    "        lock = Lock()\n",
    "        learner = mp.Process(target=learner_process, args=(i, shm.name, shape, lock, shared_epoch, done_epoch))\n",
    "        learner.start()\n",
    "\n",
    "        shared_names.append(shm.name)\n",
    "        shapes.append(shape)\n",
    "        locks.append(lock)\n",
    "        learners.append(learner)\n",
    "\n",
    "    actor_loop(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch)\n",
    "\n",
    "    for learner in learners:\n",
    "        learner.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d71dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import shared_memory, Lock, Value\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return torch.cat([p.data.view(-1) for p in self.parameters()])\n",
    "\n",
    "    def load_flat_weights(self, flat_tensor):\n",
    "        pointer = 0\n",
    "        for param in self.parameters():\n",
    "            numel = param.numel()\n",
    "            param.data.copy_(flat_tensor[pointer:pointer + numel].view_as(param))\n",
    "            pointer += numel\n",
    "\n",
    "class SharedEnv:\n",
    "    def reset(self):\n",
    "        return [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "\n",
    "    def step(self, actions):\n",
    "        next_obs = [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "        rewards = [np.random.rand() for _ in range(6)]\n",
    "        dones = [np.random.rand() > 0.95 for _ in range(6)]\n",
    "        return next_obs, rewards, dones, {}\n",
    "\n",
    "def learner_process(agent_id, shape, weight_np, lock, shared_epoch, done_epoch):\n",
    "    model = AgentNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    dummy_input = torch.randn((32, 4))\n",
    "    dummy_target = torch.randint(0, 2, (32,))\n",
    "    last_epoch = 0\n",
    "\n",
    "    while True:\n",
    "        current_epoch = shared_epoch.value\n",
    "        if current_epoch > last_epoch:\n",
    "            output = model(dummy_input)\n",
    "            loss = nn.CrossEntropyLoss()(output, dummy_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with lock:\n",
    "                weight_np[:] = model.get_flat_weights().numpy()\n",
    "            print(f\"[Learner {agent_id}] Trained for epoch {current_epoch}\")\n",
    "\n",
    "            done_epoch[agent_id] = current_epoch\n",
    "            last_epoch = current_epoch\n",
    "        time.sleep(0.05)\n",
    "\n",
    "def actor_loop(num_agents, locks, shared_epoch, done_epoch, weight_nps):\n",
    "    models = [AgentNet() for _ in range(num_agents)]\n",
    "    env = SharedEnv()\n",
    "    obs_list = env.reset()\n",
    "    epoch = 0\n",
    "    steps_per_epoch = 10\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            with locks[i]:\n",
    "                weights_tensor = torch.tensor(weight_nps[i].copy(), dtype=torch.float32)\n",
    "            models[i].load_flat_weights(weights_tensor)\n",
    "            obs_tensor = torch.tensor(obs_list[i], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action = models[i](obs_tensor).argmax().item()\n",
    "            actions.append(action)\n",
    "\n",
    "        next_obs, rewards, dones, _ = env.step(actions)\n",
    "        obs_list = next_obs\n",
    "        step_count += 1\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            print(f\"[Agent {i}] Action: {actions[i]} Reward: {rewards[i]:.2f} Done: {dones[i]}\")\n",
    "\n",
    "        if step_count >= steps_per_epoch:\n",
    "            step_count = 0\n",
    "            epoch += 1\n",
    "            shared_epoch.value = epoch\n",
    "            print(f\"[Actor] Epoch {epoch} completed. Waiting for learners...\")\n",
    "\n",
    "            while not all(done_epoch[i] == epoch for i in range(num_agents)):\n",
    "                time.sleep(0.1)\n",
    "            print(f\"[Actor] All learners completed training for epoch {epoch}\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def setup():\n",
    "    num_agents = 6\n",
    "    shared_epoch = Value('i', 0)\n",
    "    done_epoch = mp.Array('i', [0] * num_agents)\n",
    "    locks = []\n",
    "    learners = []\n",
    "    weight_nps = []\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        dummy_model = AgentNet()\n",
    "        flat_weights = dummy_model.get_flat_weights()\n",
    "        shape = flat_weights.shape\n",
    "        shm = shared_memory.SharedMemory(create=True, size=flat_weights.numel() * 4)\n",
    "        weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "        weight_np[:] = flat_weights.numpy()\n",
    "\n",
    "        lock = Lock()\n",
    "        learner = mp.Process(target=learner_process, args=(i, shape, weight_np, lock, shared_epoch, done_epoch))\n",
    "        learner.start()\n",
    "\n",
    "        locks.append(lock)\n",
    "        learners.append(learner)\n",
    "        weight_nps.append(weight_np)\n",
    "\n",
    "    return num_agents, locks, shared_epoch, done_epoch, learners, weight_nps\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     mp.set_start_method(\"spawn\")\n",
    "#     num_agents, locks, shared_epoch, done_epoch, learners, weight_nps = setup()\n",
    "#     actor_loop(num_agents, locks, shared_epoch, done_epoch, weight_nps)\n",
    "#     for learner in learners:\n",
    "#         learner.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceba537a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# mp.set_start_method(\"spawn\")\n",
    "num_agents, locks, shared_epoch, done_epoch, learners, weight_nps = setup()\n",
    "actor_loop(num_agents, locks, shared_epoch, done_epoch, weight_nps)\n",
    "for learner in learners:\n",
    "    learner.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import shared_memory, Lock, Value\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Sequence\n",
    "\n",
    "# ==== Shared Replay Buffer ====\n",
    "class SharedReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: Sequence[int], name_prefix=\"rb\"):\n",
    "        self.capacity = capacity\n",
    "        self.obs_shape = obs_shape\n",
    "        self.lock = Lock()\n",
    "\n",
    "        obs_bytes = np.prod((capacity, *obs_shape)) * np.dtype(np.float32).itemsize\n",
    "        act_bytes = capacity * np.dtype(np.int64).itemsize\n",
    "        rew_bytes = capacity * np.dtype(np.float32).itemsize\n",
    "        done_bytes = capacity * np.dtype(np.float32).itemsize\n",
    "\n",
    "        self.obs_shm = shared_memory.SharedMemory(create=True, size=obs_bytes, name=f\"{name_prefix}_obs\")\n",
    "        self.act_shm = shared_memory.SharedMemory(create=True, size=act_bytes, name=f\"{name_prefix}_act\")\n",
    "        self.rew_shm = shared_memory.SharedMemory(create=True, size=rew_bytes, name=f\"{name_prefix}_rew\")\n",
    "        self.done_shm = shared_memory.SharedMemory(create=True, size=done_bytes, name=f\"{name_prefix}_done\")\n",
    "\n",
    "        self.buffer = np.ndarray((capacity, *obs_shape), dtype=np.float32, buffer=self.obs_shm.buf)\n",
    "        self.actions = np.ndarray(capacity, dtype=np.int64, buffer=self.act_shm.buf)\n",
    "        self.rewards = np.ndarray(capacity, dtype=np.float32, buffer=self.rew_shm.buf)\n",
    "        self.dones = np.ndarray(capacity, dtype=np.float32, buffer=self.done_shm.buf)\n",
    "\n",
    "        self.idx = Value('i', 0)\n",
    "        self.size = Value('i', 0)\n",
    "\n",
    "    def add(self, obs, action, reward, done):\n",
    "        with self.lock:\n",
    "            i = self.idx.value\n",
    "            self.buffer[i] = obs\n",
    "            self.actions[i] = action\n",
    "            self.rewards[i] = reward\n",
    "            self.dones[i] = done\n",
    "            self.idx.value = (i + 1) % self.capacity\n",
    "            self.size.value = min(self.size.value + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int, stacked_frames: int = 1):\n",
    "        with self.lock:\n",
    "            size = self.size.value\n",
    "\n",
    "        indices = np.random.choice(max(1, size - stacked_frames - 1), batch_size, replace=False)\n",
    "        indices = indices[:, np.newaxis]\n",
    "        indices = (indices + np.arange(stacked_frames))\n",
    "\n",
    "        states = torch.from_numpy(self.buffer[indices]).view(batch_size, -1)\n",
    "        next_states = torch.from_numpy(self.buffer[indices + 1]).view(batch_size, -1)\n",
    "        actions = torch.from_numpy(self.actions[indices[:, -1]]).view(batch_size, -1)\n",
    "        rewards = torch.from_numpy(self.rewards[indices[:, -1]]).view(batch_size, -1)\n",
    "        dones = torch.from_numpy(self.dones[indices[:, -1]]).view(batch_size, -1)\n",
    "        valid = torch.from_numpy(1. - np.any(self.dones[indices[:, :-1]], axis=-1)).view(batch_size, -1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, valid\n",
    "\n",
    "# ==== Dummy Environment ====\n",
    "class SharedEnv:\n",
    "    def reset(self):\n",
    "        return [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "\n",
    "    def step(self, actions):\n",
    "        next_obs = [np.random.randn(4).astype(np.float32) for _ in range(6)]\n",
    "        rewards = [np.random.rand() for _ in range(6)]\n",
    "        dones = [np.random.rand() > 0.95 for _ in range(6)]\n",
    "        return next_obs, rewards, dones, {}\n",
    "\n",
    "# ==== Agent Network ====\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return torch.cat([p.data.view(-1) for p in self.parameters()])\n",
    "\n",
    "    def load_flat_weights(self, flat_tensor):\n",
    "        pointer = 0\n",
    "        for param in self.parameters():\n",
    "            numel = param.numel()\n",
    "            param.data.copy_(flat_tensor[pointer:pointer + numel].view_as(param))\n",
    "            pointer += numel\n",
    "\n",
    "# ==== Learner Process ====\n",
    "def learner_process(agent_id, shared_name, shape, lock, shared_epoch, done_epoch, buffer: SharedReplayBuffer):\n",
    "    shm = shared_memory.SharedMemory(name=shared_name)\n",
    "    weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "    model = AgentNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    last_epoch = 0\n",
    "\n",
    "    while True:\n",
    "        current_epoch = shared_epoch.value\n",
    "        if current_epoch > last_epoch:\n",
    "            try:\n",
    "                states, actions, rewards, next_states, dones, valid = buffer.sample(batch_size=32)\n",
    "                output = model(states)\n",
    "                loss = output.mean()  # Replace with real loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with lock:\n",
    "                    weight_np[:] = model.get_flat_weights().numpy()\n",
    "                print(f\"[Learner {agent_id}] Trained for epoch {current_epoch}\")\n",
    "\n",
    "                done_epoch[agent_id] = current_epoch\n",
    "                last_epoch = current_epoch\n",
    "            except Exception as e:\n",
    "                print(f\"[Learner {agent_id}] Error during training: {e}\")\n",
    "        time.sleep(0.05)\n",
    "\n",
    "# ==== Main Actor Loop ====\n",
    "def actor_loop(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch, buffers):\n",
    "    models = [AgentNet() for _ in range(num_agents)]\n",
    "    shms = [shared_memory.SharedMemory(name=shared_names[i]) for i in range(num_agents)]\n",
    "    weight_nps = [np.ndarray(shapes[i], dtype=np.float32, buffer=shms[i].buf) for i in range(num_agents)]\n",
    "\n",
    "    env = SharedEnv()\n",
    "    obs_list = env.reset()\n",
    "    epoch = 0\n",
    "    steps_per_epoch = 10\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            with locks[i]:\n",
    "                weights_tensor = torch.tensor(weight_nps[i].copy(), dtype=torch.float32)\n",
    "            models[i].load_flat_weights(weights_tensor)\n",
    "            obs_tensor = torch.tensor(obs_list[i], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action = models[i](obs_tensor).argmax().item()\n",
    "            actions.append(action)\n",
    "\n",
    "        next_obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            buffers[i].add(obs_list[i], actions[i], rewards[i], float(dones[i]))\n",
    "\n",
    "        obs_list = next_obs\n",
    "        step_count += 1\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            print(f\"[Agent {i}] Action: {actions[i]} Reward: {rewards[i]:.2f} Done: {dones[i]}\")\n",
    "\n",
    "        if step_count >= steps_per_epoch:\n",
    "            step_count = 0\n",
    "            epoch += 1\n",
    "            shared_epoch.value = epoch\n",
    "            print(f\"[Actor] Epoch {epoch} completed. Waiting for learners...\")\n",
    "\n",
    "            while not all(done_epoch[i] == epoch for i in range(num_agents)):\n",
    "                time.sleep(0.1)\n",
    "            print(f\"[Actor] All learners completed training for epoch {epoch}\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# ==== Setup ====\n",
    "def main():\n",
    "    num_agents = 6\n",
    "    shared_names = []\n",
    "    shapes = []\n",
    "    locks = []\n",
    "    learners = []\n",
    "    shared_epoch = Value('i', 0)\n",
    "    done_epoch = mp.Array('i', [0] * num_agents)\n",
    "    buffers = []\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        dummy_model = AgentNet()\n",
    "        flat_weights = dummy_model.get_flat_weights()\n",
    "        shape = flat_weights.shape\n",
    "        shm = shared_memory.SharedMemory(create=True, size=flat_weights.numel() * 4)\n",
    "        weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "        weight_np[:] = flat_weights.numpy()\n",
    "\n",
    "        buffer = SharedReplayBuffer(capacity=500, obs_shape=(4,), name_prefix=f\"rb{i}\")\n",
    "        learner = mp.Process(target=learner_process, args=(i, shm.name, shape, Lock(), shared_epoch, done_epoch, buffer))\n",
    "        learner.start()\n",
    "\n",
    "        shared_names.append(shm.name)\n",
    "        shapes.append(shape)\n",
    "        locks.append(Lock())\n",
    "        learners.append(learner)\n",
    "        buffers.append(buffer)\n",
    "\n",
    "    actor_loop(num_agents, shared_names, shapes, locks, shared_epoch, done_epoch, buffers)\n",
    "\n",
    "    for learner in learners:\n",
    "        learner.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70390831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import shared_memory, Lock, Value\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Sequence\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf89f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    110\u001b[0m     mp\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 90\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m     shared_epoch\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done_epoch[i] \u001b[38;5;241m==\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)):\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m mp_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m learners:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simple model reused for both learners\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        return torch.cat([p.data.view(-1) for p in self.parameters()])\n",
    "\n",
    "    def load_flat_weights(self, flat_tensor):\n",
    "        pointer = 0\n",
    "        for param in self.parameters():\n",
    "            numel = param.numel()\n",
    "            param.data.copy_(flat_tensor[pointer:pointer + numel].view_as(param))\n",
    "            pointer += numel\n",
    "\n",
    "# Dummy buffer for benchmarking\n",
    "class DummyBuffer:\n",
    "    def sample(self, batch_size):\n",
    "        return torch.randn(batch_size, 4), torch.randint(0, 2, (batch_size,)), torch.rand(batch_size)\n",
    "\n",
    "# Multiprocessing learner\n",
    "def mp_learner(agent_id, shared_name, shape, lock, shared_epoch, done_epoch):\n",
    "    shm = shared_memory.SharedMemory(name=shared_name)\n",
    "    weight_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "    model = AgentNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    buffer = DummyBuffer()\n",
    "    last_epoch = 0\n",
    "\n",
    "    while True:\n",
    "        if shared_epoch.value > last_epoch:\n",
    "            x, y, _ = buffer.sample(32)\n",
    "            output = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with lock:\n",
    "                weight_np[:] = model.get_flat_weights().numpy()\n",
    "            done_epoch[agent_id] = shared_epoch.value\n",
    "            last_epoch = shared_epoch.value\n",
    "        time.sleep(0.01)\n",
    "\n",
    "# Sequential training loop (inside main process)\n",
    "def sequential_learners(agent_models, buffers):\n",
    "    for model, buffer in zip(agent_models, buffers):\n",
    "        x, y, _ = buffer.sample(32)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        output = model(x)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Benchmarking setup\n",
    "def main():\n",
    "    num_agents = 6\n",
    "    dummy_model = AgentNet()\n",
    "    flat_weights = dummy_model.get_flat_weights()\n",
    "    shape = flat_weights.shape\n",
    "\n",
    "    # === Multiprocessing Setup ===\n",
    "    shared_epoch = Value('i', 0)\n",
    "    done_epoch = mp.Array('i', [0] * num_agents)\n",
    "    locks = []\n",
    "    learners = []\n",
    "    shm_names = []\n",
    "    for i in range(num_agents):\n",
    "        shm = shared_memory.SharedMemory(create=True, size=flat_weights.numel() * 4)\n",
    "        shm_np = np.ndarray(shape, dtype=np.float32, buffer=shm.buf)\n",
    "        shm_np[:] = flat_weights.numpy()\n",
    "        shm_names.append(shm.name)\n",
    "        lock = Lock()\n",
    "        p = mp.Process(target=mp_learner, args=(i, shm.name, shape, lock, shared_epoch, done_epoch))\n",
    "        p.start()\n",
    "        learners.append(p)\n",
    "        locks.append(lock)\n",
    "\n",
    "    # === Benchmark MP update time ===\n",
    "    start = time.time()\n",
    "    for epoch in range(5):\n",
    "        shared_epoch.value = epoch + 1\n",
    "        while not all(done_epoch[i] == epoch + 1 for i in range(num_agents)):\n",
    "            time.sleep(0.01)\n",
    "    mp_time = time.time() - start\n",
    "\n",
    "    for p in learners:\n",
    "        p.terminate()\n",
    "\n",
    "    # === Sequential Setup ===\n",
    "    agent_models = [AgentNet() for _ in range(num_agents)]\n",
    "    buffers = [DummyBuffer() for _ in range(num_agents)]\n",
    "\n",
    "    # === Benchmark Sequential update time ===\n",
    "    start = time.time()\n",
    "    for epoch in range(5):\n",
    "        sequential_learners(agent_models, buffers)\n",
    "    seq_time = time.time() - start\n",
    "\n",
    "    print(f\"Multiprocessing time: {mp_time:.4f} s\")\n",
    "    print(f\"Sequential time:     {seq_time:.4f} s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095f96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new buffer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from multiprocessing import Array, Value, Lock\n",
    "from ctypes import c_float, c_int, c_int64\n",
    "from typing import Sequence\n",
    "\n",
    "class ClaasyReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity: int, obs_shape: Sequence[int]):\n",
    "        self.capacity = capacity\n",
    "        self.obs_shape = obs_shape\n",
    "        self.lock = Lock()\n",
    "\n",
    "        total_obs = int(np.prod((capacity, *obs_shape)))\n",
    "        self.buffer_raw = Array(c_float, total_obs, lock=False)\n",
    "        self.actions_raw = Array(c_int64, capacity, lock=False)\n",
    "        self.rewards_raw = Array(c_float, capacity, lock=False)\n",
    "        self.dones_raw = Array(c_float, capacity, lock=False)\n",
    "\n",
    "        self.buffer = np.frombuffer(self.buffer_raw, dtype=np.float32).reshape((capacity, *obs_shape))\n",
    "        self.actions = np.frombuffer(self.actions_raw, dtype=np.int64)\n",
    "        self.rewards = np.frombuffer(self.rewards_raw, dtype=np.float32)\n",
    "        self.dones = np.frombuffer(self.dones_raw, dtype=np.float32)\n",
    "\n",
    "        self.idx = Value(c_int, 0)\n",
    "        self.size = Value(c_int, 0)\n",
    "\n",
    "    def add(self, obs, action, reward, done):\n",
    "        with self.lock:\n",
    "            i = self.idx.value\n",
    "            self.buffer[i] = obs\n",
    "            self.actions[i] = action\n",
    "            self.rewards[i] = reward\n",
    "            self.dones[i] = done\n",
    "            self.idx.value = (i + 1) % self.capacity\n",
    "            self.size.value = min(self.size.value + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int, stacked_frames: int = 1):\n",
    "        with self.lock:\n",
    "            size = self.size.value\n",
    "\n",
    "        if size <= stacked_frames + 1:\n",
    "            raise ValueError(\"Not enough transitions to sample\")\n",
    "\n",
    "        indices = np.random.choice(size - stacked_frames - 1, batch_size, replace=False)\n",
    "        indices = indices[:, np.newaxis]\n",
    "        indices = (indices + np.arange(stacked_frames))\n",
    "\n",
    "        states = torch.from_numpy(self.buffer[indices]).view(batch_size, -1)\n",
    "        next_states = torch.from_numpy(self.buffer[indices + 1]).view(batch_size, -1)\n",
    "        actions = torch.from_numpy(self.actions[indices[:, -1]]).view(batch_size, -1)\n",
    "        rewards  = torch.from_numpy(self.rewards[indices[:, -1]]).view(batch_size, -1)\n",
    "        dones = torch.from_numpy(self.dones[indices[:, -1]]).view(batch_size, -1)\n",
    "        valid = torch.from_numpy(1. - np.any(self.dones[indices[:, :-1]], axis=-1)).view(batch_size, -1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, valid\n",
    "\n",
    "    def clear(self):\n",
    "        with self.lock:\n",
    "            self.buffer.fill(0)\n",
    "            self.actions.fill(0)\n",
    "            self.rewards.fill(0)\n",
    "            self.dones.fill(0)\n",
    "            self.idx.value = 0\n",
    "            self.size.value = 0\n",
    "\n",
    "    def getidx(self):\n",
    "        return self.idx.value\n",
    "\n",
    "    def current_state(self, stacked_frames=1):\n",
    "        i = self.idx.value\n",
    "        if i < stacked_frames:\n",
    "            diff = i - stacked_frames\n",
    "            return np.concatenate((self.buffer[diff % len(self):], self.buffer[:i]))\n",
    "        return self.buffer[i - stacked_frames:i]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Buffer(capacity={self.capacity}, obs_shape={self.obs_shape})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eabd166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential] Training time for 5 epochs and 6 agents: 0.0958 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "def sequential_main():\n",
    "    num_agents = 6\n",
    "    epochs = 5\n",
    "    obs_shape = (4,)\n",
    "    capacity = 500\n",
    "    batch_size = 32\n",
    "\n",
    "    # Setup\n",
    "    buffers = [ClaasyReplayBuffer(capacity, obs_shape) for _ in range(num_agents)]\n",
    "    models = [AgentNet() for _ in range(num_agents)]\n",
    "    optimizers = [torch.optim.Adam(model.parameters(), lr=0.001) for model in models]\n",
    "\n",
    "    # Fill buffers with dummy data\n",
    "    for buffer in buffers:\n",
    "        for _ in range(capacity):\n",
    "            obs = torch.randn(obs_shape).numpy()\n",
    "            action = int(torch.randint(0, 2, (1,)).item())\n",
    "            reward = float(torch.rand(1).item())\n",
    "            done = float(torch.rand(1).item() > 0.95)\n",
    "            buffer.add(obs, action, reward, done)\n",
    "\n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for agent_id in range(num_agents):\n",
    "            model = models[agent_id]\n",
    "            optimizer = optimizers[agent_id]\n",
    "            buffer = buffers[agent_id]\n",
    "\n",
    "            states, actions, rewards, next_states, dones, valid = buffer.sample(batch_size)\n",
    "            output = model(states)\n",
    "            loss = output.mean()  # dummy loss for speed comparison\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[Sequential] Training time for {epochs} epochs and {num_agents} agents: {elapsed:.4f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sequential_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seeing_the_future",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
